{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f9651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a6b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = \"..\\\\..\\\\datasets\\\\\"\n",
    "models = \"..\\\\..\\\\models\\\\\"\n",
    "caption_model = torch.load(f\"{models}/caption_features_flickr8k.pt\")\n",
    "\n",
    "IMAGES_PATH = f\"{datasets}/flickr8k/images\"  # Directory with training images\n",
    "CAPTIONS_PATH = f\"{datasets}/flickr8k/captions.txt\"  # Caption file\n",
    "TEST_IMAGES_PATH = \"..\\\\test_images\"  # Directory with test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c88e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_caption_beam(\n",
    "    image,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    vocab,\n",
    "    beam_size=5,\n",
    "    max_len=30,\n",
    "    length_penalty=0.7\n",
    "):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    bos = vocab.word2idx[\"<start>\"]\n",
    "    eos = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    memory = encoder(image)  # [1, N, D]\n",
    "\n",
    "    beams = [(torch.tensor([[bos]], device=device), 0.0)]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score in beams:\n",
    "            if seq[0, -1].item() == eos:\n",
    "                all_candidates.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            logits = decoder(memory, seq)\n",
    "            log_probs = F.log_softmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            topk_logp, topk_idx = log_probs.topk(beam_size)\n",
    "\n",
    "            for k in range(beam_size):\n",
    "                next_seq = torch.cat(\n",
    "                    [seq, topk_idx[:, k].unsqueeze(1)], dim=1\n",
    "                )\n",
    "\n",
    "                new_score = score + topk_logp[0, k].item()\n",
    "\n",
    "                all_candidates.append((next_seq, new_score))\n",
    "\n",
    "        beams = sorted(all_candidates, key=lambda x: x[1] / (len(x[0][0]) ** length_penalty), reverse=True)[:beam_size]\n",
    "\n",
    "    best_seq = beams[0][0].squeeze(0).tolist()\n",
    "\n",
    "    caption = []\n",
    "    for idx in best_seq:\n",
    "        word = vocab.idx2word[idx]\n",
    "        if word in (\"<start>\", \"<pad>\"):\n",
    "            continue\n",
    "        if word == \"<end>\":\n",
    "            break\n",
    "        caption.append(word)\n",
    "\n",
    "    return \" \".join(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb11976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TransformerEncoderViT\n",
    "from model import TransformerCaptionDecoder\n",
    "\n",
    "vocab = torch.load(f\"{models}/vocab-VT.pkl\", weights_only=False)\n",
    "encoder = TransformerEncoderViT().to(device)\n",
    "decoder = TransformerCaptionDecoder(embed_size=256, vocab_size=len(vocab)).to(device)\n",
    "\n",
    "encoder.load_state_dict(torch.load(f\"{models}/encoder-Vit-Transformer.pth\", map_location=device))\n",
    "decoder.load_state_dict(torch.load(f\"{models}/decoder-VT.pth\", map_location=device))\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f883bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  test the model on a few images\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import csv\n",
    "\n",
    "with open(CAPTIONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)\n",
    "        seen = set()\n",
    "        for img_name, caption in reader:\n",
    "                if img_name in seen:\n",
    "                    continue\n",
    "                seen.add(img_name)\n",
    "\n",
    "                img_path = f\"{IMAGES_PATH}/{img_name}\"\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "                image = transform(image)\n",
    "                img = image.permute(1,2,0).cpu().numpy()\n",
    "                img = (img - img.min()) / (img.max() - img.min())\n",
    "                plt.imshow(img)\n",
    "\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "                #caption = generate_caption_beam(image, encoder, decoder, vocab)\n",
    "                caption = generate_caption_beam(image, encoder, decoder, vocab).strip().split()\n",
    "                print(f\"Image: {img_name}\\nCaption: {caption}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
