{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b6e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4a2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import nltk \n",
    "import tqdm\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from vocabulary_class import Vocabulary\n",
    "from rouge_score import rouge_scorer\n",
    "from jiwer import wer\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from collections import Counter\n",
    "from vocabulary_class import Vocabulary\n",
    "from flickr_dataset import FlickrDataset\n",
    "from model import TransformerEncoderViT, TransformerCaptionDecoder\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from pycocoevalcap.cider.cider import Cider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c742bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = \"..\\\\..\\\\datasets\\\\\"\n",
    "models = \"..\\\\..\\\\models\\\\\"\n",
    "caption_model = torch.load(f\"{models}/caption_features_flickr8k.pt\")\n",
    "\n",
    "IMAGES_PATH = f\"{datasets}/flickr8k/images\"  # Directory with training images\n",
    "CAPTIONS_PATH = f\"{datasets}/flickr8k/captions.txt\"  # Caption file\n",
    "TEST_IMAGES_PATH = \"..\\\\test_images\"  # Directory with test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee3d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "counter = Counter()\n",
    "\n",
    "def build_vocab(json_path, threshold=5, limit=None):\n",
    "    \n",
    "    counter = Counter()\n",
    "    image_captions = {}\n",
    "    count =0\n",
    "    with open(CAPTIONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # skip header: image,caption\n",
    "\n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue\n",
    "            img_name, caption = row\n",
    "            if img_name not in image_captions:\n",
    "                    image_captions[img_name] = []\n",
    "            image_captions[img_name].append(caption)\n",
    "\n",
    "            caption = caption.lower()\n",
    "            tokens = nltk.tokenize.word_tokenize(caption)\n",
    "            counter.update(tokens)\n",
    "            count +=1\n",
    "            if limit and count >= limit:\n",
    "                break\n",
    "    \n",
    "    vocab = Vocabulary()\n",
    "    for word, cnt in counter.items():\n",
    "        if cnt >= threshold:\n",
    "            vocab.add_word(word)\n",
    "    \n",
    "    return vocab, image_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc8cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, image_captions = build_vocab(CAPTIONS_PATH, threshold=5)\n",
    "print(\"Total vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b331ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, captions, image_ids = zip(*batch)\n",
    "\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    captions = pad_sequence(\n",
    "        captions,\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "\n",
    "    return images, captions, image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a54554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FlickrDataset(\n",
    "    root=IMAGES_PATH,\n",
    "    captions_path=CAPTIONS_PATH,\n",
    "    vocab=vocab,\n",
    "    transform=transform,\n",
    "    max_samples=None\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(len(train_dataset))\n",
    "image, caption, _ = train_dataset[0]\n",
    "\n",
    "print(type(image))\n",
    "print(image.shape)        # after transform\n",
    "print(caption)\n",
    "print(len(caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoderViT().to(device)\n",
    "decoder = TransformerCaptionDecoder(embed_size=256, vocab_size=len(vocab)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[\"<pad>\"])\n",
    "\n",
    "params = list(filter(lambda p: p.requires_grad, encoder.parameters())) + \\\n",
    "         list(filter(lambda p: p.requires_grad, decoder.parameters()))\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params,\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5295912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    total_train_loss = 0\n",
    "    for images, captions, image_ids in tqdm.tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        captions_in = captions[:, :-1]\n",
    "        targets     = captions[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        memory = encoder(images)\n",
    "        logits = decoder(memory, captions_in)\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch}: {total_train_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), f\"{models}/encoder-Vit-Transformer.pth\")\n",
    "torch.save(decoder.state_dict(), f\"{models}/decoder-VT.pth\")\n",
    "torch.save(vocab, f\"{models}/vocab-VT.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ae86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = FlickrDataset(\n",
    "    root=IMAGES_PATH,\n",
    "    captions_path=CAPTIONS_PATH,\n",
    "    vocab=vocab,\n",
    "    transform=transform,\n",
    "    split=\"val\",   # or test list\n",
    "    max_samples=1000\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    dataset=eval_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f5c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_caption_beam(\n",
    "    image,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    vocab,\n",
    "    beam_size=5,\n",
    "    max_len=30,\n",
    "    length_penalty=0.7\n",
    "):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    bos = vocab.word2idx[\"<start>\"]\n",
    "    eos = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    memory = encoder(image)  # [1, N, D]\n",
    "\n",
    "    beams = [(torch.tensor([[bos]], device=device), 0.0)]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score in beams:\n",
    "            if seq[0, -1].item() == eos:\n",
    "                all_candidates.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            logits = decoder(memory, seq)\n",
    "            log_probs = F.log_softmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            topk_logp, topk_idx = log_probs.topk(beam_size)\n",
    "\n",
    "            for k in range(beam_size):\n",
    "                next_seq = torch.cat(\n",
    "                    [seq, topk_idx[:, k].unsqueeze(1)], dim=1\n",
    "                )\n",
    "\n",
    "                new_score = score + topk_logp[0, k].item()\n",
    "\n",
    "                all_candidates.append((next_seq, new_score))\n",
    "\n",
    "        beams = sorted(all_candidates, key=lambda x: x[1] / (len(x[0][0]) ** length_penalty), reverse=True)[:beam_size]\n",
    "\n",
    "    best_seq = beams[0][0].squeeze(0).tolist()\n",
    "\n",
    "    caption = []\n",
    "    for idx in best_seq:\n",
    "        word = vocab.idx2word[idx]\n",
    "        if word in (\"<start>\", \"<pad>\"):\n",
    "            continue\n",
    "        if word == \"<end>\":\n",
    "            break\n",
    "        caption.append(word)\n",
    "\n",
    "    return \" \".join(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56efd130",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = []   # list[list[list[str]]]\n",
    "hypotheses = []   # list[list[str]]\n",
    "\n",
    "for images, _, image_ids in eval_loader:\n",
    "    for i in range(images.size(0)):\n",
    "        img_name = image_ids[i]\n",
    "\n",
    "        refs = []\n",
    "        for cap in eval_dataset.image_captions[img_name]:\n",
    "            refs.append(nltk.word_tokenize(cap.lower()))\n",
    "        references.append(refs)\n",
    "\n",
    "        image = image = images[i].to(device)\n",
    "        hyp = generate_caption_beam(image, encoder, decoder, vocab).strip().split()\n",
    "        hypotheses.append(hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f580da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HYP:\", hypotheses[0])\n",
    "print(\"REF:\", references[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avg hyp length:\", np.mean([len(h) for h in hypotheses]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2826e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu1 = corpus_bleu(references, hypotheses, weights=(1,0,0,0))\n",
    "bleu2 = corpus_bleu(references, hypotheses, weights=(0.5,0.5,0,0))\n",
    "print(\"BLEU-1:\", bleu1)\n",
    "print(\"BLEU-2:\", bleu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8462c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = SmoothingFunction().method4\n",
    "\n",
    "bleu4 = corpus_bleu(\n",
    "    references,\n",
    "    hypotheses,\n",
    "    weights=(0.25, 0.25, 0.25, 0.25),\n",
    "    smoothing_function=smooth\n",
    ")\n",
    "\n",
    "print(f\"Smoothed BLEU-4: {bleu4:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a7251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "rouge_scores = []\n",
    "\n",
    "for hyp, refs in zip(hypotheses, references):\n",
    "    hyp_str = \" \".join(hyp)\n",
    "    best = max(\n",
    "        scorer.score(\" \".join(r), hyp_str)[\"rougeL\"].fmeasure\n",
    "        for r in refs\n",
    "    )\n",
    "    rouge_scores.append(best)\n",
    "\n",
    "print(f\"ROUGE-L: {sum(rouge_scores)/len(rouge_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df1987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_wer(hyp, refs):\n",
    "    hyp_str = \" \".join(hyp)\n",
    "    return min(wer(\" \".join(r), hyp_str) for r in refs)\n",
    "\n",
    "wer_score = sum(\n",
    "    best_wer(h, r) for h, r in zip(hypotheses, references)\n",
    ") / len(hypotheses)\n",
    "\n",
    "print(f\"WER: {wer_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ec1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_dict = {i: [\" \".join(r) for r in refs] for i, refs in enumerate(references)}\n",
    "hyps_dict = {i: [\" \".join(h)] for i, h in enumerate(hypotheses)}\n",
    "\n",
    "cider, _ = Cider().compute_score(refs_dict, hyps_dict)\n",
    "print(f\"CIDEr: {cider:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
