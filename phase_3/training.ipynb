{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c742bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f695970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = \"../phase_2/data/train2014/train2014\"  # Directory with training images\n",
    "VAL_IMAGES_PATH = \"../phase_2/data/val2014/val2014\"  # Directory with validation images\n",
    "CAPTIONS_PATH = \"../phase_2/data/annotations_trainval2014/annotations/captions_train2014.json\"  # Caption file\n",
    "VAL_CAPTIONS_PATH = \"../phase_2/data/annotations_trainval2014/annotations/captions_val2014.json\"  # Validation caption file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05a33b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "414113\n",
      "Num images: 82783\n",
      "Num captions: 414113\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "coco = COCO(CAPTIONS_PATH)\n",
    "\n",
    "print(len(list(coco.anns.keys())))  # Total number of annotations\n",
    "print(\"Num images:\", len(coco.getImgIds()))\n",
    "print(\"Num captions:\", len(coco.getAnnIds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee3d968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import nltk \n",
    "from collections import Counter\n",
    "from vocabulary_class import Vocabulary\n",
    "nltk.download('punkt_tab')\n",
    "import json\n",
    "\n",
    "tokens = []\n",
    "counter = Counter()\n",
    "\n",
    "def build_vocab(json_path, threshold=5, limit=None):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f) \n",
    "\n",
    "    counter = Counter()\n",
    "    count =0\n",
    "\n",
    "    for ann in tqdm.tqdm(data['annotations']):\n",
    "        caption = ann['caption'].lower()\n",
    "        tokens = nltk.tokenize.word_tokenize(caption)\n",
    "        counter.update(tokens)\n",
    "        count +=1\n",
    "        if limit and count >= limit:\n",
    "            break\n",
    "    \n",
    "    vocab = Vocabulary()\n",
    "    for word, cnt in counter.items():\n",
    "        if cnt >= threshold:\n",
    "            vocab.add_word(word)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b105ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:17<00:00, 24247.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 8853\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(CAPTIONS_PATH, threshold=5)\n",
    "print(\"Total vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f57a7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41da85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    max_length = max(lengths)\n",
    "    padded_captions = torch.zeros(len(captions), max_length).long()\n",
    "\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        padded_captions[i, :end] = cap[:end]\n",
    "\n",
    "    return images, padded_captions, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d78ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.55s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from coco_dataset import CocoDatasetClass \n",
    "\n",
    "train_dataset = CocoDatasetClass(\n",
    "    root=IMAGES_PATH,\n",
    "    json_path=CAPTIONS_PATH,\n",
    "    vocab=vocab,\n",
    "    transform=transform,\n",
    "    max_samples=3000\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn= collate_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9638772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82783\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8966eb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AIML\\envs\\mlenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\AIML\\envs\\mlenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 2587/2587 [05:59<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 3.6718809604644775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2587/2587 [05:58<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 3.862231969833374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2587/2587 [08:57<00:00,  4.82it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 3.2863659858703613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2587/2587 [05:55<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 3.0443227291107178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2587/2587 [05:55<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 3.2176079750061035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2587/2587 [31:30<00:00,  1.37it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Loss: 3.414350748062134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2587/2587 [07:12<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 3.0578784942626953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2587/2587 [05:56<00:00,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Loss: 3.006190776824951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2587/2587 [06:16<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Loss: 3.1500911712646484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2587/2587 [11:51<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Loss: 3.201063871383667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2587/2587 [06:16<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Loss: 2.845913887023926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2587/2587 [06:48<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Loss: 2.925001859664917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2587/2587 [05:54<00:00,  7.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Loss: 3.0150487422943115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2587/2587 [05:55<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Loss: 2.621262311935425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2587/2587 [05:56<00:00,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Loss: 3.049644947052002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from model import EncoderCNN, DecoderRNN\n",
    "import torch.nn as nn\n",
    "\n",
    "encoder = EncoderCNN(embed_size=256).to(device)\n",
    "decoder = DecoderRNN(embed_size=256, hidden_size=512, vocab_size=len(vocab)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[\"<pad>\"])\n",
    "\n",
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "\n",
    "for epoch in range(15):\n",
    "    for images, captions, lengths in tqdm.tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)   # <-- IMPORTANT\n",
    "        targets = captions[:, 1:]                       # shift left\n",
    "        outputs = outputs[:, :-1, :]                    # align prediction\n",
    "        loss = criterion(outputs.reshape(-1, len(vocab)),\n",
    "                            targets.reshape(-1))\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch:\", epoch, \"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3b79618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image, encoder, decoder, vocab):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    feature = encoder(image)              # [1, 256]\n",
    "    feature = feature.unsqueeze(1)        # [1, 1, 256]\n",
    "\n",
    "    # 2. Start sequence with <start> token\n",
    "    start_token = vocab.word2idx[\"<start>\"]\n",
    "    end_token = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    sampled_ids = []\n",
    "    inputs = torch.LongTensor([[start_token]]).to(image.device)\n",
    "\n",
    "    # 3. FIRST STEP: concatenate image feature + embedding(<start>)\n",
    "    embeddings = decoder.embed(inputs)     # [1,1,256]\n",
    "    lstm_input = torch.cat((feature, embeddings), dim=1)  # [1,2,256]\n",
    "\n",
    "    hiddens, states = decoder.lstm(lstm_input)\n",
    "\n",
    "    outputs = decoder.linear(hiddens[:, -1, :])\n",
    "    predicted = outputs.argmax(dim=1).item()\n",
    "    sampled_ids.append(predicted)\n",
    "\n",
    "    # 4. NEXT STEPS: only feed predicted tokens (NO concatenation!)\n",
    "    inputs = torch.LongTensor([[predicted]]).to(image.device)\n",
    "\n",
    "    for _ in range(20):\n",
    "        embeddings = decoder.embed(inputs)  # [1,1,256]\n",
    "\n",
    "        hiddens, states = decoder.lstm(embeddings, states)\n",
    "        outputs = decoder.linear(hiddens[:, -1, :])\n",
    "        \n",
    "        predicted = outputs.argmax(dim=1).item()\n",
    "        sampled_ids.append(predicted)\n",
    "        \n",
    "        if predicted == end_token:\n",
    "            break\n",
    "\n",
    "        inputs = torch.LongTensor([[predicted]]).to(image.device)\n",
    "\n",
    "    words = [vocab.idx2word[id] for id in sampled_ids]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f77905a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.17s)\n",
      "creating index...\n",
      "index created!\n",
      "Running Evaluation on 200 validation images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40504/40504 [11:53<00:00, 56.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4 Score: 0.0012808335140070558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "coco_val = COCO(VAL_CAPTIONS_PATH)\n",
    "val_img_ids = coco_val.getImgIds()\n",
    "\n",
    "predictions = []\n",
    "references = {}\n",
    "smoothie = SmoothingFunction().method4\n",
    "scores = []\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"Running Evaluation on 200 validation images...\")\n",
    "subset_ids = val_img_ids   # Evaluate on 200 images (faster)\n",
    "\n",
    "for img_id in tqdm.tqdm(subset_ids):\n",
    "    # Load the actual image file name\n",
    "    img_info = coco_val.loadImgs(img_id)[0]\n",
    "    file_name = img_info[\"file_name\"]\n",
    "\n",
    "    # Load image\n",
    "    img_path = f\"{VAL_IMAGES_PATH}/{file_name}\"\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "\n",
    "    # print(image.shape)\n",
    "\n",
    "    # Generate prediction\n",
    "    pred_caption = generate_caption(image, encoder, decoder, vocab)\n",
    "    pred_tokens = pred_caption.lower().split()\n",
    "\n",
    "    # Ground truth captions\n",
    "    ann_ids = coco_val.getAnnIds(imgIds=img_id)\n",
    "    anns = coco_val.loadAnns(ann_ids)\n",
    "    gt_caps = [ann[\"caption\"] for ann in anns]\n",
    "\n",
    "    # Compute BLEU-4\n",
    "    bleu4 = sentence_bleu(gt_caps, pred_tokens, smoothing_function=smoothie)\n",
    "    scores.append(bleu4)\n",
    "    \n",
    "print(\"BLEU-4 Score:\", sum(scores) / len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fab1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     'encoder_state_dict': encoder.state_dict(),\n",
    "#     'decoder_state_dict': decoder.state_dict(),\n",
    "#     'vocab': vocab,\n",
    "#     'embed_size': 256,\n",
    "#     'hidden_size': 512\n",
    "# }, 'model.pth')\n",
    "\n",
    "torch.save(encoder.state_dict(), \"models/encoder.pth\")\n",
    "torch.save(decoder.state_dict(), \"models/decoder.pth\")\n",
    "torch.save(vocab, \"models/vocab.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
