{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "06c742bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f695970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGES_PATH = \"./data/train2017/train2017\"  # Directory with training images\n",
    "# VAL_IMAGES_PATH = \"./data/val2017/val2017\"  # Directory with validation images\n",
    "# CAPTIONS_PATH = \"./data/annotations_trainval2017/annotations/captions_train2017.json\"  # Caption file\n",
    "# VAL_CAPTIONS_PATH = \"./data/annotations_trainval2017/annotations/captions_val2017.json\"  # Validation caption file\n",
    "\n",
    "IMAGES_PATH = \"./data/train2014/train2014\"  # Directory with training images\n",
    "VAL_IMAGES_PATH = \"./data/val2014/val2014\"  # Directory with validation images\n",
    "CAPTIONS_PATH = \"./data/annotations_trainval2014/annotations/captions_train2014.json\"  # Caption file\n",
    "VAL_CAPTIONS_PATH = \"./data/annotations_trainval2014/annotations/captions_val2014.json\"  # Validation caption file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "05a33b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.33s)\n",
      "creating index...\n",
      "index created!\n",
      "414113\n",
      "Num images: 82783\n",
      "Num captions: 414113\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "coco = COCO(CAPTIONS_PATH)\n",
    "\n",
    "print(len(list(coco.anns.keys())))  # Total number of annotations\n",
    "print(\"Num images:\", len(coco.getImgIds()))\n",
    "print(\"Num captions:\", len(coco.getAnnIds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "aee3d968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import nltk \n",
    "from collections import Counter\n",
    "from vocabulary_class import Vocabulary\n",
    "nltk.download('punkt_tab')\n",
    "import json\n",
    "\n",
    "tokens = []\n",
    "counter = Counter()\n",
    "\n",
    "def build_vocab(json_path, threshold=5, limit=None):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f) \n",
    "\n",
    "    counter = Counter()\n",
    "    count =0\n",
    "\n",
    "    for ann in tqdm.tqdm(data['annotations']):\n",
    "        caption = ann['caption'].lower()\n",
    "        tokens = nltk.tokenize.word_tokenize(caption)\n",
    "        counter.update(tokens)\n",
    "        count +=1\n",
    "        if limit and count >= limit:\n",
    "            break\n",
    "    \n",
    "    vocab = Vocabulary()\n",
    "    for word, cnt in counter.items():\n",
    "        if cnt >= threshold:\n",
    "            vocab.add_word(word)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5b105ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:15<00:00, 26824.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 8853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(CAPTIONS_PATH, threshold=5)\n",
    "print(\"Total vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f57a7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "41da85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    max_length = max(lengths)\n",
    "    padded_captions = torch.zeros(len(captions), max_length).long()\n",
    "\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        padded_captions[i, :end] = cap[:end]\n",
    "\n",
    "    return images, padded_captions, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "103d78ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.30s)\n",
      "creating index...\n",
      "index created!\n",
      "IDs example: [57870, 384029, 222016, 520950, 69675, 547471, 122688, 392136, 398494, 90570]\n",
      "Total IDs: 82783\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from coco_dataset import CocoDatasetClass \n",
    "\n",
    "train_dataset = CocoDatasetClass(\n",
    "    root=IMAGES_PATH,\n",
    "    json_path=CAPTIONS_PATH,\n",
    "    vocab=vocab,\n",
    "    transform=transform,\n",
    "    max_samples=None\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn= collate_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e9638772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82783\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8966eb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2587 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Vocabulary' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[168], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, captions, lengths \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(train_loader):\n\u001b[0;32m     13\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m         captions \u001b[38;5;241m=\u001b[39m captions\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\AIML\\envs\\mlenv\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32md:\\AIML\\envs\\mlenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32md:\\AIML\\envs\\mlenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\AIML\\envs\\mlenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\AIML\\envs\\mlenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\AIML\\PG Course\\capstone project\\capstone\\phase_2\\coco_dataset.py:48\u001b[0m, in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     46\u001b[0m caption_ids \u001b[38;5;241m=\u001b[39m [vocab(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<start>\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     47\u001b[0m caption_ids \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [vocab(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m---> 48\u001b[0m caption_ids\u001b[38;5;241m.\u001b[39mappend(vocab(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<end>\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     50\u001b[0m caption_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(caption_ids)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, caption_tensor\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Vocabulary' object is not callable"
     ]
    }
   ],
   "source": [
    "from model import EncoderCNN, DecoderRNN\n",
    "import torch.nn as nn\n",
    "\n",
    "encoder = EncoderCNN(embed_size=256).to(device)\n",
    "decoder = DecoderRNN(embed_size=256, hidden_size=512, vocab_size=len(vocab)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[\"<pad>\"])\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for images, captions, lengths in tqdm.tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        loss = criterion(outputs.reshape(-1, len(vocab)), captions.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch:\", epoch, \"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b79618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image, encoder, decoder, vocab):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    feature = encoder(image)              # [1, 256]\n",
    "    feature = feature.unsqueeze(1)        # [1, 1, 256]\n",
    "\n",
    "    # 2. Start sequence with <start> token\n",
    "    start_token = vocab.word2idx[\"<start>\"]\n",
    "    end_token = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    sampled_ids = []\n",
    "    inputs = torch.LongTensor([[start_token]]).to(image.device)\n",
    "\n",
    "    # 3. FIRST STEP: concatenate image feature + embedding(<start>)\n",
    "    embeddings = decoder.embed(inputs)     # [1,1,256]\n",
    "    lstm_input = torch.cat((feature, embeddings), dim=1)  # [1,2,256]\n",
    "\n",
    "    hiddens, states = decoder.lstm(lstm_input)\n",
    "\n",
    "    outputs = decoder.linear(hiddens[:, -1, :])\n",
    "    predicted = outputs.argmax(dim=1).item()\n",
    "    sampled_ids.append(predicted)\n",
    "\n",
    "    # 4. NEXT STEPS: only feed predicted tokens (NO concatenation!)\n",
    "    inputs = torch.LongTensor([[predicted]]).to(image.device)\n",
    "\n",
    "    for _ in range(20):\n",
    "        embeddings = decoder.embed(inputs)  # [1,1,256]\n",
    "\n",
    "        hiddens, states = decoder.lstm(embeddings, states)\n",
    "        outputs = decoder.linear(hiddens[:, -1, :])\n",
    "        \n",
    "        predicted = outputs.argmax(dim=1).item()\n",
    "        sampled_ids.append(predicted)\n",
    "        \n",
    "        if predicted == end_token:\n",
    "            break\n",
    "\n",
    "        inputs = torch.LongTensor([[predicted]]).to(image.device)\n",
    "\n",
    "    words = [vocab.idx2word[id] for id in sampled_ids]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77905a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.23s)\n",
      "creating index...\n",
      "index created!\n",
      "Running Evaluation on 200 validation images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 38.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4 Score: 0.002009378918434691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "coco_val = COCO(VAL_CAPTIONS_PATH)\n",
    "val_img_ids = coco_val.getImgIds()\n",
    "\n",
    "predictions = []\n",
    "references = {}\n",
    "smoothie = SmoothingFunction().method4\n",
    "scores = []\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"Running Evaluation on 200 validation images...\")\n",
    "subset_ids = val_img_ids[:10]   # Evaluate on 200 images (faster)\n",
    "\n",
    "for img_id in tqdm.tqdm(subset_ids):\n",
    "    # Load the actual image file name\n",
    "    img_info = coco_val.loadImgs(img_id)[0]\n",
    "    file_name = img_info[\"file_name\"]\n",
    "\n",
    "    # Load image\n",
    "    img_path = f\"{VAL_IMAGES_PATH}/{file_name}\"\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "\n",
    "    # print(image.shape)\n",
    "\n",
    "    # Generate prediction\n",
    "    pred_caption = generate_caption(image, encoder, decoder, vocab)\n",
    "    pred_tokens = pred_caption.lower().split()\n",
    "\n",
    "    # Ground truth captions\n",
    "    ann_ids = coco_val.getAnnIds(imgIds=img_id)\n",
    "    anns = coco_val.loadAnns(ann_ids)\n",
    "    gt_caps = [ann[\"caption\"] for ann in anns]\n",
    "\n",
    "    # Compute BLEU-4\n",
    "    bleu4 = sentence_bleu(gt_caps, pred_tokens, smoothing_function=smoothie)\n",
    "    scores.append(bleu4)\n",
    "    \n",
    "print(\"BLEU-4 Score:\", sum(scores) / len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     'encoder_state_dict': encoder.state_dict(),\n",
    "#     'decoder_state_dict': decoder.state_dict(),\n",
    "#     'vocab': vocab,\n",
    "#     'embed_size': 256,\n",
    "#     'hidden_size': 512\n",
    "# }, 'model.pth')\n",
    "\n",
    "torch.save(encoder.state_dict(), \"models/encoder.pth\")\n",
    "torch.save(decoder.state_dict(), \"models/decoder.pth\")\n",
    "torch.save(vocab, \"models/vocab.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
