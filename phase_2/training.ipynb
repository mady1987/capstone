{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "06c742bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f695970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = \"./data/train2017/train2017\"  # Directory with training images\n",
    "VAL_IMAGES_PATH = \"./data/val2017/val2017\"  # Directory with validation images\n",
    "CAPTIONS_PATH = \"./data/annotations_trainval2017/annotations/captions_train2017.json\"  # Caption file\n",
    "VAL_CAPTIONS_PATH = \"./data/annotations_trainval2017/annotations/captions_val2017.json\"  # Validation caption file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "05a33b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.47s)\n",
      "creating index...\n",
      "index created!\n",
      "591753\n",
      "Num images: 118287\n",
      "Num captions: 591753\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "coco = COCO(CAPTIONS_PATH)\n",
    "\n",
    "print(len(list(coco.anns.keys())))  # Total number of annotations\n",
    "print(\"Num images:\", len(coco.getImgIds()))\n",
    "print(\"Num captions:\", len(coco.getAnnIds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aee3d968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import nltk \n",
    "from collections import Counter\n",
    "from vocabulary_class import Vocabulary\n",
    "nltk.download('punkt_tab')\n",
    "import json\n",
    "\n",
    "tokens = []\n",
    "counter = Counter()\n",
    "\n",
    "def build_vocab(json_path, threshold=5, limit =50000):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f) \n",
    "\n",
    "    counter = Counter()\n",
    "    count =0\n",
    "\n",
    "    for ann in tqdm.tqdm(data['annotations']):\n",
    "        caption = ann['caption'].lower()\n",
    "        tokens = nltk.tokenize.word_tokenize(caption)\n",
    "        counter.update(tokens)\n",
    "        count +=1\n",
    "        if count >= limit:\n",
    "            break\n",
    "    \n",
    "    vocab = Vocabulary()\n",
    "    for word, cnt in counter.items():\n",
    "        if cnt >= threshold:\n",
    "            vocab.add_word(word)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5b105ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 49999/591753 [00:01<00:20, 26666.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 3171\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(CAPTIONS_PATH, threshold=5, limit=50000)\n",
    "print(\"Total vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f57a7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "41da85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    max_length = max(lengths)\n",
    "    padded_captions = torch.zeros(len(captions), max_length).long()\n",
    "\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        padded_captions[i, :end] = cap[:end]\n",
    "\n",
    "    return images, padded_captions, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d78ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.49s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset import CocoDataset\n",
    "\n",
    "train_dataset = CocoDataset(\n",
    "    root=IMAGES_PATH,\n",
    "    json_path=CAPTIONS_PATH,\n",
    "    vocab=vocab,\n",
    "    transform=transform,\n",
    "    max_samples=50000\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn= collate_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966eb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AIML\\envs\\mlenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\AIML\\envs\\mlenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 1563/1563 [03:54<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 2.497539758682251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [03:30<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 2.3247783184051514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [03:54<00:00,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 2.1093697547912598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from model import EncoderCNN, DecoderRNN\n",
    "import torch.nn as nn\n",
    "\n",
    "encoder = EncoderCNN(embed_size=256).to(device)\n",
    "decoder = DecoderRNN(embed_size=256, hidden_size=512, vocab_size=len(vocab)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[\"<pad>\"])\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for images, captions, lengths in tqdm.tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        loss = criterion(outputs.reshape(-1, len(vocab)), captions.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch:\", epoch, \"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c3b79618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image, encoder, decoder, vocab):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    feature = encoder(image)              # [1, 256]\n",
    "    feature = feature.unsqueeze(1)        # [1, 1, 256]\n",
    "\n",
    "    # 2. Start sequence with <start> token\n",
    "    start_token = vocab.word2idx[\"<start>\"]\n",
    "    end_token = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    sampled_ids = []\n",
    "    inputs = torch.LongTensor([[start_token]]).to(image.device)\n",
    "\n",
    "    # 3. FIRST STEP: concatenate image feature + embedding(<start>)\n",
    "    embeddings = decoder.embed(inputs)     # [1,1,256]\n",
    "    lstm_input = torch.cat((feature, embeddings), dim=1)  # [1,2,256]\n",
    "\n",
    "    hiddens, states = decoder.lstm(lstm_input)\n",
    "\n",
    "    outputs = decoder.linear(hiddens[:, -1, :])\n",
    "    predicted = outputs.argmax(dim=1).item()\n",
    "    sampled_ids.append(predicted)\n",
    "\n",
    "    # 4. NEXT STEPS: only feed predicted tokens (NO concatenation!)\n",
    "    inputs = torch.LongTensor([[predicted]]).to(image.device)\n",
    "\n",
    "    for _ in range(20):\n",
    "        embeddings = decoder.embed(inputs)  # [1,1,256]\n",
    "\n",
    "        hiddens, states = decoder.lstm(embeddings, states)\n",
    "        outputs = decoder.linear(hiddens[:, -1, :])\n",
    "        \n",
    "        predicted = outputs.argmax(dim=1).item()\n",
    "        sampled_ids.append(predicted)\n",
    "        \n",
    "        if predicted == end_token:\n",
    "            break\n",
    "\n",
    "        inputs = torch.LongTensor([[predicted]]).to(image.device)\n",
    "\n",
    "    words = [vocab.idx2word[id] for id in sampled_ids]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f77905a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running Evaluation on 200 validation images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 59.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4 Score: 0.002388656114150767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "coco_val = COCO(VAL_CAPTIONS_PATH)\n",
    "val_img_ids = coco_val.getImgIds()\n",
    "\n",
    "predictions = []\n",
    "references = {}\n",
    "smoothie = SmoothingFunction().method4\n",
    "scores = []\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"Running Evaluation on 200 validation images...\")\n",
    "subset_ids = val_img_ids[:10]   # Evaluate on 200 images (faster)\n",
    "\n",
    "for img_id in tqdm.tqdm(subset_ids):\n",
    "    # Load the actual image file name\n",
    "    img_info = coco_val.loadImgs(img_id)[0]\n",
    "    file_name = img_info[\"file_name\"]\n",
    "\n",
    "    # Load image\n",
    "    img_path = f\"{VAL_IMAGES_PATH}/{file_name}\"\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "\n",
    "    # print(image.shape)\n",
    "\n",
    "    # Generate prediction\n",
    "    pred_caption = generate_caption(image, encoder, decoder, vocab)\n",
    "    pred_tokens = pred_caption.lower().split()\n",
    "\n",
    "    # Ground truth captions\n",
    "    ann_ids = coco_val.getAnnIds(imgIds=img_id)\n",
    "    anns = coco_val.loadAnns(ann_ids)\n",
    "    gt_caps = [ann[\"caption\"] for ann in anns]\n",
    "\n",
    "    # Compute BLEU-4\n",
    "    bleu4 = sentence_bleu(gt_caps, pred_tokens, smoothing_function=smoothie)\n",
    "    scores.append(bleu4)\n",
    "    \n",
    "print(\"BLEU-4 Score:\", sum(scores) / len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1fab1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     'encoder_state_dict': encoder.state_dict(),\n",
    "#     'decoder_state_dict': decoder.state_dict(),\n",
    "#     'vocab': vocab,\n",
    "#     'embed_size': 256,\n",
    "#     'hidden_size': 512\n",
    "# }, 'model.pth')\n",
    "\n",
    "torch.save(encoder.state_dict(), \"models/encoder.pth\")\n",
    "torch.save(decoder.state_dict(), \"models/decoder.pth\")\n",
    "torch.save(vocab, \"models/vocab.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
