{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c742bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eac745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from flickr_dataset import FlickrDataset \n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import torch.nn as nn\n",
    "from vocabulary_class import Vocabulary\n",
    "from rouge_score import rouge_scorer\n",
    "from jiwer import wer\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5259398",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = \"..\\\\..\\\\datasets\\\\\"\n",
    "models = \"..\\\\..\\\\models\\\\\"\n",
    "caption_model = torch.load(f\"{models}/caption_features_flickr8k.pt\")\n",
    "\n",
    "IMAGES_PATH = f\"{datasets}/flickr8k/images\"  # Directory with training images\n",
    "CAPTIONS_PATH = f\"{datasets}/flickr8k/captions.txt\"  # Caption file\n",
    "TEST_IMAGES_PATH = \"..\\\\test_images\"  # Directory with test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee3d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import nltk \n",
    "from collections import Counter\n",
    "from vocabulary_class import Vocabulary\n",
    "nltk.download('punkt_tab')\n",
    "import json\n",
    "\n",
    "tokens = []\n",
    "counter = Counter()\n",
    "\n",
    "import csv\n",
    "\n",
    "def build_vocab(json_path, threshold=5, limit=None):\n",
    "    \n",
    "    counter = Counter()\n",
    "    image_captions = {}\n",
    "    count =0\n",
    "    with open(CAPTIONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # skip header: image,caption\n",
    "\n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue\n",
    "            img_name, caption = row\n",
    "            if img_name not in image_captions:\n",
    "                    image_captions[img_name] = []\n",
    "            image_captions[img_name].append(caption)\n",
    "\n",
    "            caption = caption.lower()\n",
    "            tokens = nltk.tokenize.word_tokenize(caption)\n",
    "            counter.update(tokens)\n",
    "            count +=1\n",
    "            if limit and count >= limit:\n",
    "                break\n",
    "    \n",
    "    vocab = Vocabulary()\n",
    "    for word, cnt in counter.items():\n",
    "        if cnt >= threshold:\n",
    "            vocab.add_word(word)\n",
    "    \n",
    "    return vocab, image_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b105ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, image_captions = build_vocab(CAPTIONS_PATH, threshold=5)\n",
    "print(\"Total vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380f2811",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    lengths = torch.tensor([len(c) for c in captions])\n",
    "\n",
    "    captions = pad_sequence(\n",
    "        captions,\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "\n",
    "    return images, captions, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d78ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FlickrDataset(\n",
    "    root=IMAGES_PATH,\n",
    "    captions_path=CAPTIONS_PATH,\n",
    "    vocab=vocab,\n",
    "    transform=transform,\n",
    "    max_samples=None\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn= collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9638772",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebfabd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, caption = train_dataset[0]\n",
    "\n",
    "print(type(image))\n",
    "print(image.shape)        # after transform\n",
    "print(caption)\n",
    "print(len(caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderCNN(embed_size=256).to(device)\n",
    "decoder = DecoderRNN(embed_size=256, hidden_size=512, vocab_size=len(vocab)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=vocab.word2idx[\"<pad>\"],\n",
    "    label_smoothing=0.1\n",
    ")\n",
    "\n",
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": encoder.parameters(), \"lr\": 1e-4},   # ViT: small LR\n",
    "    {\"params\": decoder.parameters(), \"lr\": 1e-3},   # LSTM: larger LR\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5295912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "for epoch in range(20):\n",
    "    total_train_loss = 0\n",
    "    for images, captions, lengths in tqdm.tqdm(train_loader):\n",
    "\n",
    "        if epoch < 3:\n",
    "            for p in encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        else:\n",
    "            for p in encoder.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)   # <-- IMPORTANT\n",
    "        targets = captions[:, 1:]                       # shift left\n",
    "        outputs = outputs[:, :-1, :]                    # align prediction\n",
    "        loss = criterion(outputs.reshape(-1, len(vocab)),\n",
    "                            targets.reshape(-1))\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch}: Train={avg_train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b79618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image, encoder, decoder, vocab):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    feature = encoder(image)              # [1, 256]\n",
    "    feature = feature.unsqueeze(1)        # [1, 1, 256]\n",
    "\n",
    "    # 2. Start sequence with <start> token\n",
    "    start_token = vocab.word2idx[\"<start>\"]\n",
    "    end_token = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    sampled_ids = []\n",
    "    inputs = torch.LongTensor([[start_token]]).to(image.device)\n",
    "\n",
    "    # 3. FIRST STEP: concatenate image feature + embedding(<start>)\n",
    "    embeddings = decoder.embed(inputs)     # [1,1,256]\n",
    "    lstm_input = torch.cat((feature, embeddings), dim=1)  # [1,2,256]\n",
    "\n",
    "    hiddens, states = decoder.lstm(lstm_input)\n",
    "\n",
    "    outputs = decoder.linear(hiddens[:, -1, :])\n",
    "    predicted = outputs.argmax(dim=1).item()\n",
    "    sampled_ids.append(predicted)\n",
    "\n",
    "    # 4. NEXT STEPS: only feed predicted tokens (NO concatenation!)\n",
    "    inputs = torch.LongTensor([[predicted]]).to(image.device)\n",
    "\n",
    "    for _ in range(20):\n",
    "        embeddings = decoder.embed(inputs)  # [1,1,256]\n",
    "\n",
    "        hiddens, states = decoder.lstm(embeddings, states)\n",
    "        outputs = decoder.linear(hiddens[:, -1, :])\n",
    "        \n",
    "        predicted = outputs.argmax(dim=1).item()\n",
    "        sampled_ids.append(predicted)\n",
    "        \n",
    "        if predicted == end_token:\n",
    "            break\n",
    "\n",
    "        inputs = torch.LongTensor([[predicted]]).to(image.device)\n",
    "\n",
    "    words = [vocab.idx2word[id] for id in sampled_ids]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), f\"{models}//encoder-resnet50.pth\")\n",
    "torch.save(decoder.state_dict(), f\"{models}//decoder-rrnn-lstm.pth\")\n",
    "torch.save(vocab, f\"{models}//vocab.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8887d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab = torch.load(f\"{models}/vocab.pkl\", weights_only=False)\n",
    "encoder = EncoderCNN(256).to(device)\n",
    "decoder = DecoderRNN(\n",
    "    256,\n",
    "    512,\n",
    "    len(vocab)\n",
    ").to(device)\n",
    "\n",
    "encoder.load_state_dict(torch.load(f\"{models}/encoder-resnet50.pth\", weights_only=True, map_location=device))\n",
    "decoder.load_state_dict(torch.load(f\"{models}/decoder-rrnn-lstm.pth\", weights_only=True, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d7ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(tokens):\n",
    "    if isinstance(tokens, list):\n",
    "        return \" \".join(tokens)\n",
    "    return tokens\n",
    "\n",
    "def compute_rouge_l(references, hypotheses):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for refs, hyp in zip(references, hypotheses):\n",
    "        hyp_str = tokens_to_string(hyp)\n",
    "\n",
    "        rouge_l_scores = []\n",
    "        for ref in refs:\n",
    "            ref_str = tokens_to_string(ref)\n",
    "            score = scorer.score(ref_str, hyp_str)['rougeL'].fmeasure\n",
    "            rouge_l_scores.append(score)\n",
    "\n",
    "        scores.append(max(rouge_l_scores))  # best reference\n",
    "\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "def compute_wer(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Computes WER using best matching reference per hypothesis\n",
    "    \"\"\"\n",
    "    wers = []\n",
    "    ref_tokens = []\n",
    "    for refs in references:\n",
    "        processed_refs = []\n",
    "        for ref in refs:\n",
    "            if isinstance(ref, list):\n",
    "                processed_refs.append(ref)        # already tokenized\n",
    "            else:\n",
    "                processed_refs.append(ref.split()) # string → tokens\n",
    "        ref_tokens.append(processed_refs)\n",
    "\n",
    "    # Handle hypotheses\n",
    "    hyp_tokens = []\n",
    "    for hyp in hypotheses:\n",
    "        if isinstance(hyp, list):\n",
    "            hyp_tokens.append(hyp)\n",
    "        else:\n",
    "            hyp_tokens.append(hyp.split())\n",
    "\n",
    "    for refs, hyp in zip(references, hypotheses):\n",
    "        ref_wers = [wer(ref, hyp) for ref in refs]\n",
    "        wers.append(min(ref_wers))  # Best match\n",
    "\n",
    "    return sum(wers) / len(wers)\n",
    "\n",
    "def compute_bleu4(references, hypotheses):\n",
    "    \"\"\"\n",
    "    references: list of list of reference captions\n",
    "    hypotheses: list of predicted captions\n",
    "    \"\"\"\n",
    "    ref_tokens = []\n",
    "    for refs in references:\n",
    "        processed_refs = []\n",
    "        for ref in refs:\n",
    "            if isinstance(ref, list):\n",
    "                processed_refs.append(ref)        # already tokenized\n",
    "            else:\n",
    "                processed_refs.append(ref.split()) # string → tokens\n",
    "        ref_tokens.append(processed_refs)\n",
    "\n",
    "    # Handle hypotheses\n",
    "    hyp_tokens = []\n",
    "    for hyp in hypotheses:\n",
    "        if isinstance(hyp, list):\n",
    "            hyp_tokens.append(hyp)\n",
    "        else:\n",
    "            hyp_tokens.append(hyp.split())\n",
    "\n",
    "\n",
    "    smoothie = SmoothingFunction().method4\n",
    "\n",
    "    bleu4 = corpus_bleu(\n",
    "        ref_tokens,\n",
    "        hyp_tokens,\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),\n",
    "        # weights=(0, 0, 0, 0),\n",
    "        smoothing_function=smoothie\n",
    "    )\n",
    "\n",
    "    return bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af98c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bluen score caluclation\n",
    "references = []\n",
    "hypotheses = []\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    for images, captions, lengths in tqdm.tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)   # <-- IMPORTANT\n",
    "\n",
    "        _, predicted = outputs.max(2)  # [B, max_len]\n",
    "\n",
    "        for i in range(captions.size(0)):\n",
    "            ref = []\n",
    "            for j in range(1, lengths[i]):  # remove <start> token\n",
    "                ref.append(vocab.idx2word[captions[i][j].item()])\n",
    "            references.append([ref])\n",
    "\n",
    "            hyp = []\n",
    "            for j in range(1, lengths[i]):  # remove <start> token\n",
    "                hyp.append(vocab.idx2word[predicted[i][j-1].item()])\n",
    "            hypotheses.append(hyp)\n",
    "        \n",
    "bleu4 = compute_bleu4(references, hypotheses)\n",
    "print(f\"blue 4 scores:: {bleu4:.4f}\")\n",
    "\n",
    "rougue_score = compute_rouge_l(references, hypotheses)\n",
    "print(f\"Rouge Score: {rougue_score:.4f}\")\n",
    "\n",
    "wer_score = compute_wer(references, hypotheses)\n",
    "print(f\"WER Score: {wer_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d87bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  test the model on a few images\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "for file_name in os.listdir(TEST_IMAGES_PATH)[:20]:\n",
    "    # Load image\n",
    "    img_path = f\"{TEST_IMAGES_PATH}/{file_name}\"\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    image = transform(image)\n",
    "    plt.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    caption = generate_caption(image, encoder, decoder, vocab)\n",
    "    print(f\"Image: {file_name}\\nCaption: {caption}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
