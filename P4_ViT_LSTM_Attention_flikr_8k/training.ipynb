{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c742bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import nltk \n",
    "import tqdm\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from vocabulary_class import Vocabulary\n",
    "from rouge_score import rouge_scorer\n",
    "from jiwer import wer\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from collections import Counter\n",
    "from P2_Resnet_LSTM_flikr_8k.vocabulary_class import Vocabulary\n",
    "from P2_Resnet_LSTM_flikr_8k.flickr_dataset import FlickrDataset\n",
    "from model import AttentionEncoderViT, AttentionDecoderRNN, TextEncoder\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c8b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = \"..\\\\..\\\\datasets\\\\\"\n",
    "models = \"..\\\\..\\\\models\\\\\"\n",
    "caption_model = torch.load(f\"{models}/caption_features_flickr8k.pt\")\n",
    "\n",
    "IMAGES_PATH = f\"{datasets}/flickr8k/images\"  # Directory with training images\n",
    "CAPTIONS_PATH = f\"{datasets}/flickr8k/captions.txt\"  # Caption file\n",
    "TEST_IMAGES_PATH = \"..\\\\test_images\"  # Directory with test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee3d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "counter = Counter()\n",
    "\n",
    "import csv\n",
    "\n",
    "def build_vocab(json_path, threshold=5, limit=None):\n",
    "    \n",
    "    counter = Counter()\n",
    "    image_captions = {}\n",
    "    count =0\n",
    "    with open(CAPTIONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # skip header: image,caption\n",
    "\n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue\n",
    "            img_name, caption = row\n",
    "            if img_name not in image_captions:\n",
    "                    image_captions[img_name] = []\n",
    "            image_captions[img_name].append(caption)\n",
    "\n",
    "            caption = caption.lower()\n",
    "            tokens = nltk.tokenize.word_tokenize(caption)\n",
    "            counter.update(tokens)\n",
    "            count +=1\n",
    "            if limit and count >= limit:\n",
    "                break\n",
    "    \n",
    "    vocab = Vocabulary()\n",
    "    for word, cnt in counter.items():\n",
    "        if cnt >= threshold:\n",
    "            vocab.add_word(word)\n",
    "    \n",
    "    return vocab, image_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b105ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, image_captions = build_vocab(CAPTIONS_PATH, threshold=5)\n",
    "print(\"Total vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380f2811",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "    images, captions, image_ids = zip(*batch)\n",
    "\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    padded_caps = torch.zeros(len(captions), max_len).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        padded_caps[i, :end] = cap[:end]\n",
    "\n",
    "    return images, padded_caps, lengths, image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d78ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FlickrDataset(\n",
    "    root=IMAGES_PATH,\n",
    "    captions_path=CAPTIONS_PATH,\n",
    "    vocab=vocab,\n",
    "    transform=transform,\n",
    "    max_samples=None\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,      # ⬅️ IMPORTANT\n",
    "    shuffle=True,\n",
    "    num_workers=0,     # ⬅️ CRITICAL FOR WINDOWS\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=False   # ⬅️ disable for debugging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4de4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, captions, lengths = next(iter(train_loader))\n",
    "print(images.shape, captions.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9638772",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebfabd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, caption = train_dataset[0]\n",
    "\n",
    "print(type(image))\n",
    "print(image.shape)        # after transform\n",
    "print(caption)\n",
    "print(len(caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = AttentionEncoderViT().to(device)\n",
    "\n",
    "decoder = AttentionDecoderRNN(\n",
    "    embed_size=256,\n",
    "    hidden_size=512,\n",
    "    vocab_size=len(vocab),\n",
    "    encoder_dim=768\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=vocab.word2idx[\"<pad>\"],\n",
    "    label_smoothing=0.1\n",
    ")\n",
    "\n",
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": encoder.parameters(), \"lr\": 5e-5},\n",
    "    {\"params\": decoder.parameters(), \"lr\": 1e-3},\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5295912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "total_epochs = 15\n",
    "\n",
    "for p in encoder.vit.blocks[-2].parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    teacher_forcing_ratio = max(0.9 * (0.95 ** epoch), 0.1)\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for images, captions, lengths in tqdm.tqdm(train_loader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        captions = captions.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            features = encoder(images)\n",
    "\n",
    "            outputs = decoder(\n",
    "                features,\n",
    "                captions,\n",
    "                lengths=lengths,\n",
    "                teacher_forcing_ratio=teacher_forcing_ratio\n",
    "            )\n",
    "            targets = captions[:, 1:]\n",
    "            targets = targets[:, :outputs.size(1)]\n",
    "\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, len(vocab)),\n",
    "                targets.reshape(-1)\n",
    "            )\n",
    "\n",
    "        # ✅ AMP-safe backward\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{total_epochs}] | \"\n",
    "        f\"Loss: {avg_train_loss:.4f} | \"\n",
    "        f\"Teacher forcing: {teacher_forcing_ratio:.3f}\"\n",
    "    )\n",
    "\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b79618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption_beam(\n",
    "    image,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    vocab,\n",
    "    beam_size=5,\n",
    "    max_len=20\n",
    "):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    encoder_out = encoder(image)  # [1, N, 256]\n",
    "\n",
    "    start = vocab.word2idx[\"<start>\"]\n",
    "    end = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    num_patches = encoder_out.size(1)  # e.g. 196\n",
    "    coverage0 = torch.zeros(1, num_patches).to(device)\n",
    "\n",
    "    sequences = [[ [start], 0.0, None, None, coverage0 ]]\n",
    "\n",
    "    completed = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score, h, c, coverage in sequences:\n",
    "            if seq[-1] == end:\n",
    "                completed.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            inputs = torch.LongTensor([[seq[-1]]]).to(device)\n",
    "            emb = decoder.embed(inputs).squeeze(1)\n",
    "\n",
    "            if h is None:\n",
    "                h0 = torch.zeros(1, decoder.lstm.hidden_size).to(device)\n",
    "                c0 = torch.zeros_like(h0)\n",
    "\n",
    "                context, alpha, coverage = decoder.attention(\n",
    "                    encoder_out, h0, coverage\n",
    "                )\n",
    "                lstm_input = torch.cat((emb, context), dim=1)\n",
    "                h, c = decoder.lstm(lstm_input, (h0, c0))\n",
    "            else:\n",
    "                context, alpha, coverage = decoder.attention(\n",
    "                    encoder_out, h, coverage\n",
    "                )\n",
    "                lstm_input = torch.cat((emb, context), dim=1)\n",
    "                h, c = decoder.lstm(lstm_input, (h, c))\n",
    "\n",
    "            logits = decoder.fc(h)\n",
    "            log_probs = torch.log_softmax(logits, dim=1)\n",
    "\n",
    "            topk = torch.topk(log_probs, beam_size)\n",
    "\n",
    "            for i in range(beam_size):\n",
    "                token = topk.indices[0][i].item()\n",
    "                log_prob = topk.values[0][i].item()\n",
    "\n",
    "                length_penalty = 0.7\n",
    "                new_score = (score + log_prob) / ((len(seq) + 1) ** length_penalty)\n",
    "\n",
    "                all_candidates.append([\n",
    "                    seq + [token],\n",
    "                    new_score,\n",
    "                    h,\n",
    "                    c,\n",
    "                    coverage\n",
    "                ])\n",
    "\n",
    "        sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "\n",
    "    completed += [(seq, score) for seq, score, _, _, _ in sequences]\n",
    "    best_seq = max(completed, key=lambda x: x[1])[0]\n",
    "\n",
    "    return \" \".join([vocab.idx2word[i] for i in best_seq if i not in [start, end]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), f\"{models}/encoder-Vit-LSTM-Attention.pth\")\n",
    "torch.save(decoder.state_dict(), f\"{models}/decoder-VLA.pth\")\n",
    "torch.save(vocab, f\"{models}/vocab-VLA.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8887d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torch.load(f\"{models}/vocab-VLA.pkl\", weights_only=False)\n",
    "encoder = AttentionEncoderViT().to(device)\n",
    "\n",
    "decoder = AttentionDecoderRNN(\n",
    "    embed_size=256,\n",
    "    hidden_size=512,\n",
    "    vocab_size=len(vocab),\n",
    "    encoder_dim=768\n",
    ").to(device)\n",
    "\n",
    "encoder.load_state_dict(torch.load(f\"{models}/encoder-Vit-LSTM-Attention.pth\", weights_only=True, map_location=device))\n",
    "decoder.load_state_dict(torch.load(f\"{models}/decoder-VLA.pth\", weights_only=True, map_location=device), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d481ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def tokens_to_string(tokens):\n",
    "    if isinstance(tokens, list):\n",
    "        return \" \".join(tokens)\n",
    "    return tokens\n",
    "\n",
    "def compute_rouge_l(references, hypotheses):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for refs, hyp in zip(references, hypotheses):\n",
    "        hyp_str = tokens_to_string(hyp)\n",
    "\n",
    "        rouge_l_scores = []\n",
    "        for ref in refs:\n",
    "            ref_str = tokens_to_string(ref)\n",
    "            score = scorer.score(ref_str, hyp_str)['rougeL'].fmeasure\n",
    "            rouge_l_scores.append(score)\n",
    "\n",
    "        scores.append(max(rouge_l_scores))  # best reference\n",
    "\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "from jiwer import wer\n",
    "def compute_wer(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Computes WER using best matching reference per hypothesis\n",
    "    \"\"\"\n",
    "    wers = []\n",
    "    ref_tokens = []\n",
    "    for refs in references:\n",
    "        processed_refs = []\n",
    "        for ref in refs:\n",
    "            if isinstance(ref, list):\n",
    "                processed_refs.append(ref)        # already tokenized\n",
    "            else:\n",
    "                processed_refs.append(ref.split()) # string → tokens\n",
    "        ref_tokens.append(processed_refs)\n",
    "\n",
    "    # Handle hypotheses\n",
    "    hyp_tokens = []\n",
    "    for hyp in hypotheses:\n",
    "        if isinstance(hyp, list):\n",
    "            hyp_tokens.append(hyp)\n",
    "        else:\n",
    "            hyp_tokens.append(hyp.split())\n",
    "\n",
    "    for refs, hyp in zip(references, hypotheses):\n",
    "        ref_wers = [wer(ref, hyp) for ref in refs]\n",
    "        wers.append(min(ref_wers))  # Best match\n",
    "\n",
    "    return sum(wers) / len(wers)\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "def compute_bleu4(references, hypotheses):\n",
    "    \"\"\"\n",
    "    references: list of list of reference captions\n",
    "    hypotheses: list of predicted captions\n",
    "    \"\"\"\n",
    "    ref_tokens = []\n",
    "    for refs in references:\n",
    "        processed_refs = []\n",
    "        for ref in refs:\n",
    "            if isinstance(ref, list):\n",
    "                processed_refs.append(ref)        # already tokenized\n",
    "            else:\n",
    "                processed_refs.append(ref.split()) # string → tokens\n",
    "        ref_tokens.append(processed_refs)\n",
    "\n",
    "    # Handle hypotheses\n",
    "    hyp_tokens = []\n",
    "    for hyp in hypotheses:\n",
    "        if isinstance(hyp, list):\n",
    "            hyp_tokens.append(hyp)\n",
    "        else:\n",
    "            hyp_tokens.append(hyp.split())\n",
    "\n",
    "\n",
    "    smoothie = SmoothingFunction().method4\n",
    "\n",
    "    bleu4 = corpus_bleu(\n",
    "        ref_tokens,\n",
    "        hyp_tokens,\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),\n",
    "        # weights=(0, 0, 0, 0),\n",
    "        smoothing_function=smoothie\n",
    "    )\n",
    "\n",
    "    return bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda07717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bluen score caluclation\n",
    "def wer_best_reference(hyp_tokens, refs_tokens):\n",
    "    hyp = \" \".join(hyp_tokens)   # ✅ tokens → string\n",
    "    return min(\n",
    "        wer(\" \".join(ref), hyp)  # ✅ tokens → string\n",
    "        for ref in refs_tokens\n",
    "    )\n",
    "\n",
    "references = []\n",
    "hypotheses = []\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Build reference captions per image\n",
    "image_to_refs = {}\n",
    "\n",
    "for img_name, caps in train_dataset.image_captions.items():\n",
    "    refs = []\n",
    "    for cap in caps:\n",
    "        tokens = nltk.tokenize.word_tokenize(cap.lower())\n",
    "        refs.append(tokens)\n",
    "    image_to_refs[img_name] = refs\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, captions, lengths, image_ids in tqdm.tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            hyp = generate_caption_beam(\n",
    "                images[i],\n",
    "                encoder,\n",
    "                decoder,\n",
    "                vocab,\n",
    "                beam_size=5,\n",
    "                max_len=20\n",
    "            ).split()\n",
    "\n",
    "            img_name = image_ids[i]\n",
    "            refs = image_to_refs[img_name]   # ✅ ALL 5 captions\n",
    "\n",
    "            references.append(refs)\n",
    "            hypotheses.append(hyp)\n",
    "      \n",
    "bleu4 = compute_bleu4(references, hypotheses)\n",
    "print(f\"blue 4 scores:: {bleu4:.4f}\")\n",
    "\n",
    "rougue_score = compute_rouge_l(references, hypotheses)\n",
    "print(f\"Rouge Score: {rougue_score:.4f}\")\n",
    "\n",
    "wers = []\n",
    "\n",
    "for hyp, refs in zip(hypotheses, references):\n",
    "    wers.append(wer_best_reference(hyp, refs))\n",
    "\n",
    "final_wer = sum(wers) / len(wers)\n",
    "print(f\"WER Score: {final_wer:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7faa3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(similarity, k):\n",
    "    \"\"\"\n",
    "    similarity: [N, N] similarity matrix\n",
    "    \"\"\"\n",
    "    topk = similarity.topk(k, dim=1).indices\n",
    "    targets = torch.arange(similarity.size(0)).unsqueeze(1).to(similarity.device)\n",
    "    correct = (topk == targets).any(dim=1)\n",
    "    return correct.float().mean().item()\n",
    "\n",
    "def extract_image_embeddings(dataloader, encoder, device):\n",
    "    encoder.eval()\n",
    "    image_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ , lengths in dataloader:\n",
    "            images = images.to(device)\n",
    "            patch_emb = encoder(images)               # [B, 196, 256]\n",
    "            feats = patch_emb.mean(dim=1)            # [B, 256]\n",
    "            feats = F.normalize(feats, dim=1)\n",
    "            image_embeddings.append(feats)\n",
    "\n",
    "    return torch.cat(image_embeddings, dim=0)       # [N, embed_size]\n",
    "\n",
    "def extract_text_embeddings(dataloader, text_encoder, device):\n",
    "    text_encoder.eval()\n",
    "    all_sent = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, captions, lengths in dataloader:\n",
    "            captions = captions.to(device)\n",
    "            lengths = torch.tensor(lengths)\n",
    "\n",
    "            sent = text_encoder(captions, lengths)\n",
    "            all_sent.append(sent.cpu())\n",
    "\n",
    "    return torch.cat(all_sent, dim=0)\n",
    "\n",
    "def image_to_text_retrieval(image_emb, text_emb, batch_size=512):\n",
    "    image_emb = image_emb.cuda()\n",
    "    text_emb  = text_emb.cuda()\n",
    "\n",
    "    N = image_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        img_batch = image_emb[i:i+batch_size]           # [B, D]\n",
    "        sim = img_batch @ text_emb.T                     # [B, N]\n",
    "\n",
    "        gt = torch.arange(i, min(i+batch_size, N)).cuda()\n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j in range(sorted_idx.size(0)):\n",
    "            rank = (sorted_idx[j] == gt[j]).nonzero(as_tuple=True)[0].item()\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = torch.tensor(ranks)\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }\n",
    "\n",
    "def text_to_image_retrieval(image_emb, text_emb, batch_size=512):\n",
    "    image_emb = image_emb.cuda()\n",
    "    text_emb  = text_emb.cuda()\n",
    "\n",
    "    N = text_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        txt_batch = text_emb[i:i+batch_size]           # [B, D]\n",
    "        sim = txt_batch @ image_emb.T                     # [B, N]\n",
    "\n",
    "        gt = torch.arange(i, min(i+batch_size, N)).cuda()\n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j in range(sorted_idx.size(0)):\n",
    "            rank = (sorted_idx[j] == gt[j]).nonzero(as_tuple=True)[0].item()\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = torch.tensor(ranks)\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }\n",
    "\n",
    "def text_to_text_retrieval(text_emb, batch_size=512):\n",
    "    text_emb  = text_emb.cuda()\n",
    "\n",
    "    N = text_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        txt_batch = text_emb[i:i+batch_size]           # [B, D]\n",
    "        sim = txt_batch @ text_emb.T                     # [B, N]\n",
    "        sim.fill_diagonal_(-1)\n",
    "\n",
    "        gt = torch.arange(i, min(i+batch_size, N)).cuda()\n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j in range(sorted_idx.size(0)):\n",
    "            rank = (sorted_idx[j] == gt[j]).nonzero(as_tuple=True)[0].item()\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = torch.tensor(ranks)\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }\n",
    "\n",
    "def image_to_image_retrieval(image_emb):\n",
    "    sim = image_emb @ image_emb.t()\n",
    "\n",
    "    # Remove self-matching\n",
    "    sim.fill_diagonal_(-1)\n",
    "\n",
    "    return {\n",
    "        \"R@1\": recall_at_k(sim, 1),\n",
    "        \"R@5\": recall_at_k(sim, 5),\n",
    "        \"R@10\": recall_at_k(sim, 10),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b188d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_emb = extract_image_embeddings(train_loader, encoder, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d04b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_size=256,\n",
    "    hidden_size=512\n",
    ").to(device)\n",
    "\n",
    "text_emb = extract_text_embeddings(train_loader, text_encoder, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b1f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_emb.shape)  # [num_images, 256]\n",
    "print(text_emb.shape)   # [num_images*5, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a960a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image → Text:\", image_to_text_retrieval(image_emb, text_emb))\n",
    "print(\"Text → Image:\", text_to_image_retrieval(image_emb, text_emb))\n",
    "print(\"Text → Text :\", text_to_text_retrieval(text_emb))\n",
    "print(\"Image → Image:\", image_to_image_retrieval(image_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d87bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  test the model on a few images\n",
    "for file_name in os.listdir(TEST_IMAGES_PATH)[:20]:\n",
    "    # Load image\n",
    "    img_path = f\"{TEST_IMAGES_PATH}/{file_name}\"\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    image = transform(image)\n",
    "    plt.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    caption = generate_caption_beam(image, encoder, decoder, vocab)\n",
    "    print(f\"Image: {file_name}\\nCaption: {caption}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
