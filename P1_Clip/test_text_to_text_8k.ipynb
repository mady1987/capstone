{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cabf0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import clip\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca7184",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = \"..\\\\..\\\\datasets\\\\\"\n",
    "models = \"..\\\\..\\\\models\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c37151",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d553668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model = torch.load(f\"{models}/caption_features_flickr8k.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034fa0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_embs = torch.stack(list(caption_model.values()))  # [8000, 512]\n",
    "caption_img_names = [cap[0] for cap in caption_model]\n",
    "print(f\"Number of captions: {len(caption_img_names)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c680e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_embs = caption_embs.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae48ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image to image\n",
    "test_text_feature = []\n",
    "\n",
    "# prepocess the text and move it to the device\n",
    "test_image_preprocessed = \"A black and white dog is running through the grass\"\n",
    "\n",
    "text_input = clip.tokenize([test_image_preprocessed], truncate=True).to(device)\n",
    "\n",
    "# Get the image feature using the model.encode_image function\n",
    "with torch.no_grad():\n",
    "    test_text_feature = model.encode_text(text_input) # output shape torch.Size([1, 512])\n",
    "\n",
    "    # Normalize the image feature\n",
    "    test_text_feature /= test_text_feature.norm(dim=-1, keepdim=True)\n",
    "\n",
    "test_text_feature.shape # torch.Size([1, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a11edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_text_feature.shape)     #torch.Size([1, 512])\n",
    "print(caption_embs.squeeze(1).shape) #torch.Size([40455, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPTIONS_PATH = f\"{datasets}/flickr8k/captions.txt\" \n",
    "captions = []\n",
    "\n",
    "with open(CAPTIONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # skip header: image,caption\n",
    "\n",
    "    for row in reader:\n",
    "        if len(row) < 2:\n",
    "            continue\n",
    "        img_name, caption = row\n",
    "        captions.append((img_name.strip(), caption.strip()))\n",
    "\n",
    "print(\"Total captions:\", len(captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = F.cosine_similarity(test_text_feature, caption_embs.squeeze(1).to(device)).squeeze()\n",
    "print(sims.shape)  #torch.Size([40455])\n",
    "\n",
    "# Get the top 5 most similar images\n",
    "topk = sims.topk(5).indices\n",
    "\n",
    "# Display the top 5 most similar images\n",
    "print(topk)\n",
    "print(topk.shape)\n",
    "\n",
    "retrieved_text = [captions[j] for j in topk]\n",
    "print(f\"input_text: {test_image_preprocessed}\")\n",
    "print(\"Retrieved captions:\")\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "i = 0\n",
    "for img_name, caption in retrieved_text:\n",
    "    print(f\"{img_name}: {caption}\")\n",
    "    \n",
    "    img_path = os.path.join(f\"{datasets}/flickr8k/Images\", img_name)\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Sim: {sims[topk[i]]:.4f}\")\n",
    "    i += 1\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
