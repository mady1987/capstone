{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "12b68b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4d96a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = \"../phase_2/data/val2014/val2014\"  # Directory with training images\n",
    "CAPTIONS_PATH = \"../phase_2/data/annotations_trainval2014/annotations/captions_val2014.json\"  # Caption file\n",
    "TEST_IMAGES_PATH = \"../phase_1/test copy/\"  # Directory with test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4cbfa33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/madhu-\n",
      "[nltk_data]     thiramdas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import nltk \n",
    "from collections import Counter\n",
    "from vocabulary_class import Vocabulary\n",
    "nltk.download('punkt')\n",
    "import json\n",
    "\n",
    "tokens = []\n",
    "counter = Counter()\n",
    "\n",
    "import csv\n",
    "import string\n",
    "\n",
    "tokens = []\n",
    "counter = Counter()\n",
    "\n",
    "def build_vocab(json_path, threshold=5, limit=None):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f) \n",
    "\n",
    "    counter = Counter()\n",
    "    count =0\n",
    "\n",
    "    for ann in tqdm.tqdm(data['annotations']):\n",
    "        caption = ann['caption'].lower()\n",
    "        tokens = nltk.tokenize.word_tokenize(caption)\n",
    "        counter.update(tokens)\n",
    "        count +=1\n",
    "        if limit and count >= limit:\n",
    "            break\n",
    "    \n",
    "    vocab = Vocabulary()\n",
    "    for word, cnt in counter.items():\n",
    "        if cnt >= threshold:\n",
    "            vocab.add_word(word)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cf9695df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202654/202654 [00:08<00:00, 25111.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 6507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(CAPTIONS_PATH, threshold=5)\n",
    "print(\"Total vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "82246f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "\n",
    "class CocoDatasetClass(Dataset):\n",
    "    def __init__(self, root, json_path, vocab, transform=None, max_samples=None):\n",
    "        self.root = root\n",
    "        self.coco = COCO(json_path)\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.ids = self.coco.getImgIds()\n",
    "        \n",
    "        self.transform = transform\n",
    "        if max_samples:\n",
    "            self.ids = self.ids[:max_samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vocab = self.vocab\n",
    "        img_id = self.ids[index]\n",
    "        \n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.root, img_info['file_name'])\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            # skip corrupted image\n",
    "            return self.__getitem__((index + 1) % len(self.ids))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        caption = random.choice(anns)['caption'].lower()\n",
    "        \n",
    "        tokens = nltk.tokenize.word_tokenize(caption)\n",
    "\n",
    "        caption_tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption_indices = [self.vocab.word2idx[\"<start>\"]]\n",
    "        caption_indices += [self.vocab.word2idx.get(token, self.vocab.word2idx[\"<unk>\"]) for token in caption_tokens]\n",
    "        caption_indices.append(self.vocab.word2idx[\"<end>\"])\n",
    "\n",
    "        caption_tensor = torch.tensor(caption_indices)\n",
    "\n",
    "        return image, caption_tensor, img_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "38723c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.12s)\n",
      "creating index...\n",
      "index created!\n",
      "40504\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 224, 224])\n",
      "tensor([   1,    4,  136,    7,    4,  260,  944,   54,    4,   50, 1182,   54,\n",
      "           4,  553,  130,   13,    2])\n",
      "17\n",
      "391895\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, captions , ids= zip(*batch)\n",
    "\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    captions = pad_sequence(\n",
    "        captions,\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "\n",
    "    return images, captions, ids\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataset = CocoDatasetClass(\n",
    "    root=IMAGES_PATH,\n",
    "    json_path=CAPTIONS_PATH,\n",
    "    vocab=vocab,\n",
    "    transform=transform,\n",
    "    max_samples=None\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=12,\n",
    "    collate_fn= collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "print(len(test_dataset))\n",
    "image, caption , ids= test_dataset[0]\n",
    "\n",
    "print(type(image))\n",
    "print(image.shape)        # after transform\n",
    "print(caption)\n",
    "print(len(caption))\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a798a4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import TransformerEncoderViT\n",
    "from model import TransformerDecoder\n",
    "\n",
    "vocab = torch.load(\"models/vocab.pkl\", weights_only=False)\n",
    "encoder = TransformerEncoderViT(256).to(device)\n",
    "decoder = TransformerDecoder(embed_size=256, vocab_size=len(vocab)).to(device)\n",
    "\n",
    "encoder.load_state_dict(torch.load(\"models/encoder.pth\", map_location=device))\n",
    "decoder.load_state_dict(torch.load(\"models/decoder.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "10a4bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def tokens_to_string(tokens):\n",
    "    if isinstance(tokens, list):\n",
    "        return \" \".join(tokens)\n",
    "    return tokens\n",
    "\n",
    "def compute_rouge_l(references, hypotheses):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for refs, hyp in zip(references, hypotheses):\n",
    "        hyp_str = tokens_to_string(hyp)\n",
    "\n",
    "        rouge_l_scores = []\n",
    "        for ref in refs:\n",
    "            ref_str = tokens_to_string(ref)\n",
    "            score = scorer.score(ref_str, hyp_str)['rougeL'].fmeasure\n",
    "            rouge_l_scores.append(score)\n",
    "\n",
    "        scores.append(max(rouge_l_scores))  # best reference\n",
    "\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "from jiwer import wer\n",
    "def compute_wer(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Computes WER using best matching reference per hypothesis\n",
    "    \"\"\"\n",
    "    wers = []\n",
    "    ref_tokens = []\n",
    "    for refs in references:\n",
    "        processed_refs = []\n",
    "        for ref in refs:\n",
    "            if isinstance(ref, list):\n",
    "                processed_refs.append(ref)        # already tokenized\n",
    "            else:\n",
    "                processed_refs.append(ref.split()) # string → tokens\n",
    "        ref_tokens.append(processed_refs)\n",
    "\n",
    "    # Handle hypotheses\n",
    "    hyp_tokens = []\n",
    "    for hyp in hypotheses:\n",
    "        if isinstance(hyp, list):\n",
    "            hyp_tokens.append(hyp)\n",
    "        else:\n",
    "            hyp_tokens.append(hyp.split())\n",
    "\n",
    "    for refs, hyp in zip(references, hypotheses):\n",
    "        ref_wers = [wer(ref, hyp) for ref in refs]\n",
    "        wers.append(min(ref_wers))  # Best match\n",
    "\n",
    "    return sum(wers) / len(wers)\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "def clean_caption(tokens, special_tokens={\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"}):\n",
    "    return [w for w in tokens if w not in special_tokens]\n",
    "\n",
    "def compute_bleu_scores(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    dataloader,\n",
    "    vocab,\n",
    "    device,\n",
    "    decode_fn,          # greedy or beam decode function\n",
    "    max_len=30\n",
    "):\n",
    "    \"\"\"\n",
    "    references: list of list of reference captions\n",
    "    hypotheses: list of predicted captions\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    references = []   # list of list of list of words\n",
    "    hypotheses = []   # list of list of words\n",
    "\n",
    "    for images, captions, ids in dataloader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            image = images[i]\n",
    "\n",
    "            # ---- generate caption ----\n",
    "            pred_sentence = decode_fn(\n",
    "                image=image,\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                vocab=vocab,\n",
    "                max_len=max_len\n",
    "            )\n",
    "\n",
    "            pred_tokens = clean_caption(pred_sentence.split())\n",
    "            hypotheses.append(pred_tokens)\n",
    "\n",
    "            # ---- SINGLE reference caption ----\n",
    "            cap_tokens = captions[i].tolist()  # [max_len]\n",
    "            ref_words = [\n",
    "                vocab.idx2word[idx]\n",
    "                for idx in cap_tokens\n",
    "                if idx not in (\n",
    "                    vocab.word2idx[\"<start>\"],\n",
    "                    vocab.word2idx[\"<end>\"],\n",
    "                    vocab.word2idx[\"<pad>\"],\n",
    "                    vocab.word2idx[\"<unk>\"]\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            references.append([ref_words])  # <-- note: list of list\n",
    "\n",
    "     # ---- BLEU scores ----\n",
    "    # bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
    "    # bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
    "    # bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "    return {\n",
    "        # \"BLEU-1\": bleu1,\n",
    "        # \"BLEU-2\": bleu2,\n",
    "        # \"BLEU-3\": bleu3,\n",
    "        \"BLEU-4\": bleu4\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ceb588e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode_transformer(image, encoder, decoder, vocab, max_len=30):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    bos = vocab.word2idx[\"<start>\"]\n",
    "    eos = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    image = image.unsqueeze(0)\n",
    "    memory = encoder(image)\n",
    "\n",
    "    generated = torch.tensor([[bos]], device=image.device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits = decoder(memory, generated)\n",
    "        next_token = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == eos:\n",
    "            break\n",
    "\n",
    "    words = []\n",
    "    for idx in generated.squeeze(0).tolist():\n",
    "        w = vocab.idx2word[idx]\n",
    "        if w in (\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"):\n",
    "            continue\n",
    "        words.append(w)\n",
    "\n",
    "    return \" \".join(words)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "46b8049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode_transformer(image, encoder, decoder, vocab, max_len=30):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    bos = vocab.word2idx[\"<start>\"]\n",
    "    eos = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    image = image.unsqueeze(0)\n",
    "    memory = encoder(image)\n",
    "\n",
    "    generated = torch.tensor([[bos]], device=image.device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits = decoder(memory, generated)\n",
    "        next_token = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == eos:\n",
    "            break\n",
    "\n",
    "    words = []\n",
    "    for idx in generated.squeeze(0).tolist():\n",
    "        w = vocab.idx2word[idx]\n",
    "        if w in (\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"):\n",
    "            continue\n",
    "        words.append(w)\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ca27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores = compute_bleu_scores(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    dataloader=test_loader,\n",
    "    vocab=vocab,\n",
    "    device=device,\n",
    "    decode_fn=greedy_decode_transformer,\n",
    "    max_len=30\n",
    ")\n",
    "\n",
    "print(bleu_scores) \n",
    "# {'BLEU-4': 1.8037836476501788e-79}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1f49abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "def recall_at_k(similarity, k):\n",
    "    \"\"\"\n",
    "    similarity: [N, N] similarity matrix\n",
    "    \"\"\"\n",
    "    topk = similarity.topk(k, dim=1).indices\n",
    "    targets = torch.arange(similarity.size(0)).unsqueeze(1).to(similarity.device)\n",
    "    correct = (topk == targets).any(dim=1)\n",
    "    return correct.float().mean().item()\n",
    "\n",
    "def extract_image_embeddings(dataloader, encoder, device):\n",
    "    encoder.eval()\n",
    "    image_embeddings = []\n",
    "    all_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ , image_ids in dataloader:\n",
    "            images = images.to(device)\n",
    "            patch_emb = encoder(images)               # [B, 196, 256]\n",
    "            feats = patch_emb.mean(dim=1)            # [B, 256]\n",
    "            feats = F.normalize(feats, dim=1)\n",
    "            image_embeddings.append(feats)\n",
    "            all_ids.extend(image_ids) \n",
    "\n",
    "    return torch.cat(image_embeddings, dim=0) , all_ids      # [N, embed_size]\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_text_embeddings(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    dataloader,\n",
    "    vocab,\n",
    "    device\n",
    "):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_ids = []\n",
    "\n",
    "    pad_idx = vocab.word2idx[\"<pad>\"]\n",
    "\n",
    "    for images, captions, image_ids in dataloader:\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # ---- Encode images ----\n",
    "        memory = encoder(images)  # [B, N, D]\n",
    "\n",
    "        # ---- Transformer decoder forward (replicates decoder internals) ----\n",
    "        B, T = captions.shape\n",
    "        device = captions.device\n",
    "        positions = torch.arange(T, device=device).unsqueeze(0)\n",
    "\n",
    "        x = decoder.embed(captions) + decoder.pos_embed(positions)\n",
    "        x = x * math.sqrt(decoder.embed_size)\n",
    "\n",
    "        attn_mask = nn.Transformer.generate_square_subsequent_mask(T).to(captions.device)\n",
    "\n",
    "        padding_mask = (captions == pad_idx)\n",
    "\n",
    "        out = decoder.decoder(\n",
    "            tgt=x,\n",
    "            memory=memory,\n",
    "            tgt_mask=attn_mask,\n",
    "            tgt_key_padding_mask=padding_mask,\n",
    "            tgt_is_causal=True \n",
    "        )  # [B, T, D]\n",
    "\n",
    "        # ---- extract last valid token per sentence ----\n",
    "        lengths = (~padding_mask).sum(dim=1) - 1\n",
    "        sent_emb = out[torch.arange(B), lengths]  # [B, D]\n",
    "\n",
    "        sent_emb = F.normalize(sent_emb, dim=1)\n",
    "\n",
    "        all_embeddings.append(sent_emb.cpu())\n",
    "        all_ids.extend(image_ids)\n",
    "\n",
    "    return torch.cat(all_embeddings, dim=0), all_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "448b0edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image_to_text_retrieval(image_emb, text_emb, image_ids, text_ids, batch_size=512):\n",
    "    image_emb = image_emb.cuda()\n",
    "    text_emb  = text_emb.cuda()\n",
    "\n",
    "     # build ID → index map for text embeddings\n",
    "    text_id_to_index = {tid: idx for idx, tid in enumerate(text_ids)}\n",
    "\n",
    "    N = image_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        img_batch = image_emb[i:i+batch_size]           # [B, D]\n",
    "        sim = img_batch @ text_emb.T                     # [B, N]\n",
    "\n",
    "        batch_image_ids = image_ids[i:i+batch_size]\n",
    "        \n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j, img_id in enumerate(batch_image_ids):\n",
    "            gt_index = text_id_to_index[img_id]\n",
    "\n",
    "            rank = (sorted_idx[j] == gt_index).nonzero(as_tuple=True)[0].item()\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = torch.tensor(ranks, device=image_emb.device)\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }\n",
    "\n",
    "def text_to_image_retrieval(image_emb, text_emb, image_ids, text_ids, batch_size=512):\n",
    "    text_emb  = text_emb.cuda()\n",
    "    image_emb = image_emb.cuda()\n",
    "\n",
    "    # build ID → index map for images\n",
    "    image_id_to_index = {iid: idx for idx, iid in enumerate(image_ids)}\n",
    "\n",
    "    N = text_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        txt_batch = text_emb[i:i+batch_size]     # [B, D]\n",
    "        sim = txt_batch @ image_emb.T             # [B, N_image]\n",
    "\n",
    "        batch_text_ids = text_ids[i:i+batch_size]\n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j, txt_id in enumerate(batch_text_ids):\n",
    "            gt_index = image_id_to_index[txt_id]  # ✅ correct GT image\n",
    "\n",
    "            rank = (sorted_idx[j] == gt_index).nonzero(as_tuple=True)[0].item()\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = torch.tensor(ranks, device=text_emb.device)\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }\n",
    "\n",
    "def text_to_text_retrieval(text_emb, text_ids, batch_size=512):\n",
    "    text_emb = text_emb.cuda()\n",
    "\n",
    "    # build image_id → list of text indices\n",
    "    id_to_indices = {}\n",
    "    for idx, img_id in enumerate(text_ids):\n",
    "        id_to_indices.setdefault(img_id, []).append(idx)\n",
    "\n",
    "    N = text_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        txt_batch = text_emb[i:i+batch_size]     # [B, D]\n",
    "        sim = txt_batch @ text_emb.T              # [B, N]\n",
    "\n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j in range(sorted_idx.size(0)):\n",
    "            query_idx = i + j\n",
    "            img_id = text_ids[query_idx]\n",
    "\n",
    "            # valid GT indices = same image_id, excluding itself\n",
    "            gt_indices = [idx for idx in id_to_indices[img_id] if idx != query_idx]\n",
    "\n",
    "            if len(gt_indices) == 0:\n",
    "                continue \n",
    "            # find best (lowest) rank among all GT indices\n",
    "            rank = min(\n",
    "                (sorted_idx[j] == gt).nonzero(as_tuple=True)[0].item()\n",
    "                for gt in gt_indices\n",
    "            )\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = torch.tensor(ranks, device=text_emb.device)\n",
    "\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }\n",
    "\n",
    "def image_to_image_retrieval(image_emb, image_ids, batch_size=512):\n",
    "    \"\"\"\n",
    "    image_emb : Tensor [N, D]\n",
    "    image_ids : list of image_ids\n",
    "    \"\"\"\n",
    "\n",
    "    image_emb = image_emb.cuda()\n",
    "\n",
    "    # build image_id → list of indices\n",
    "    id_to_indices = {}\n",
    "    for idx, img_id in enumerate(image_ids):\n",
    "        id_to_indices.setdefault(img_id, []).append(idx)\n",
    "\n",
    "    N = image_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        img_batch = image_emb[i:i+batch_size]    # [B, D]\n",
    "        sim = img_batch @ image_emb.T             # [B, N]\n",
    "\n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j in range(sorted_idx.size(0)):\n",
    "            query_idx = i + j\n",
    "            img_id = image_ids[query_idx]\n",
    "\n",
    "            # same ID images excluding itself\n",
    "            gt_indices = [idx for idx in id_to_indices[img_id] if idx != query_idx]\n",
    "\n",
    "            if len(gt_indices) == 0:\n",
    "                continue  # only one image per ID → skip\n",
    "\n",
    "            rank = min(\n",
    "                (sorted_idx[j] == gt).nonzero(as_tuple=True)[0].item()\n",
    "                for gt in gt_indices\n",
    "            )\n",
    "            ranks.append(rank)\n",
    "\n",
    "    if len(ranks) == 0:\n",
    "        return {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0}\n",
    "\n",
    "    ranks = torch.tensor(ranks, device=image_emb.device)\n",
    "\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9ada6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_caption_transformer(image, encoder, decoder, vocab, max_len=30):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    bos = vocab.word2idx[\"<start>\"]\n",
    "    eos = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    memory = encoder(image)  # [1, N, D]\n",
    "\n",
    "    generated = torch.tensor([[bos]], device=device)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        logits = decoder(memory, generated)      # [1, t, V]\n",
    "        next_token = logits[:, -1].argmax(dim=-1, keepdim=True)  # [1,1]\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == eos:\n",
    "            break\n",
    "\n",
    "    words = []\n",
    "    for idx in generated.squeeze(0).tolist():\n",
    "        w = vocab.idx2word[idx]\n",
    "        if w in (\"<start>\", \"<pad>\"):\n",
    "            continue\n",
    "        if w == \"<end>\":\n",
    "            break\n",
    "        words.append(w)\n",
    "\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8e76b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_emb, image_ids  = extract_image_embeddings(test_loader, encoder, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5dda0540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madhu-thiramdas/ai-work/ai-venv-py310/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text_emb, text_ids = extract_text_embeddings(encoder, decoder, test_loader, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "29b7f8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image_ids: [391895, 522418, 184613, 318219, 554625]\n",
      "Sample text_ids : [391895, 522418, 184613, 318219, 554625]\n",
      "image_ids unique: 40504\n",
      "text_ids unique : 40504\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample image_ids:\", image_ids[:5])\n",
    "print(\"Sample text_ids :\", text_ids[:5])\n",
    "print(\"image_ids unique:\", len(set(image_ids)))\n",
    "print(\"text_ids unique :\", len(set(text_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "395b8c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image → Text: {'R@1': 2.4688919438631274e-05, 'R@5': 0.00012344459537416697, 'R@10': 0.0003456448612269014}\n",
      "Text → Image: {'R@1': 0.0, 'R@5': 0.00012344459537416697, 'R@10': 0.00022220028040464967}\n",
      "Text → Text : {'R@1': nan, 'R@5': nan, 'R@10': nan}\n",
      "Image → Image: {'R@1': 0.0, 'R@5': 0.0, 'R@10': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Image → Text:\", image_to_text_retrieval(image_emb, text_emb, image_ids, text_ids))\n",
    "print(\"Text → Image:\", text_to_image_retrieval(image_emb, text_emb, image_ids, text_ids))\n",
    "print(\"Text → Text :\", text_to_text_retrieval(text_emb, text_ids))\n",
    "print(\"Image → Image:\", image_to_image_retrieval(image_emb, image_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Python 3.10 (RTX 5070)",
   "language": "python",
   "name": "ai-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
