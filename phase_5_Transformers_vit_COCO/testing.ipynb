{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12b68b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d96a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = \"../phase_1/data/flickr30k_images/flickr30k_images\"  # Directory with training images\n",
    "CAPTIONS_PATH = \"../phase_1/data/flickr30k_images/results_testing.csv\"  # Caption file\n",
    "TEST_IMAGES_PATH = \"../phase_1/test copy/\"  # Directory with test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbfa33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import nltk \n",
    "from collections import Counter\n",
    "from vocabulary_class import Vocabulary\n",
    "nltk.download('punkt')\n",
    "import json\n",
    "\n",
    "tokens = []\n",
    "counter = Counter()\n",
    "\n",
    "import csv\n",
    "import string\n",
    "\n",
    "def build_vocab(captions_path, threshold=3, limit=None):\n",
    "    \n",
    "    counter = Counter()\n",
    "    image_captions = {}\n",
    "    count =0\n",
    "    with open(captions_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        # reader = csv.reader(f)\n",
    "        # next(reader)  # skip header: image,caption\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # Skip header\n",
    "            if line.startswith(\"image_name\"):\n",
    "                continue\n",
    "\n",
    "            # Split by '|'\n",
    "            parts = [p.strip() for p in line.split(\"|\")]\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "\n",
    "            img_name,_, caption = parts\n",
    "            if img_name not in image_captions:\n",
    "                    image_captions[img_name] = []\n",
    "            image_captions[img_name].append(caption)\n",
    "\n",
    "            caption = caption.lower()\n",
    "            tokens = [\n",
    "                token for token in nltk.tokenize.word_tokenize(caption)\n",
    "                if token not in string.punctuation\n",
    "            ]\n",
    "            if len(tokens) < 1:\n",
    "                continue\n",
    "            counter.update(tokens)\n",
    "            if limit and len(image_captions) >= limit:\n",
    "                break\n",
    "    \n",
    "    vocab = Vocabulary()\n",
    "    for word, cnt in counter.items():\n",
    "        if cnt >= threshold:\n",
    "            vocab.add_word(word)\n",
    "    \n",
    "\n",
    "    return vocab, image_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf9695df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vocab, image_captions \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCAPTIONS_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal vocabulary size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(vocab))\n",
      "Cell \u001b[1;32mIn[40], line 25\u001b[0m, in \u001b[0;36mbuild_vocab\u001b[1;34m(json_path, threshold, limit)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(row) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m img_name, caption \u001b[38;5;241m=\u001b[39m row\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m image_captions:\n\u001b[0;32m     27\u001b[0m         image_captions[img_name] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "vocab, image_captions = build_vocab(CAPTIONS_PATH, threshold=5)\n",
    "print(\"Total vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82246f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Update the folowing datset call class to handle flikr8k dataset formate\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "\n",
    "class FlickrDataset30K(Dataset):\n",
    "    def __init__(self, root, captions_path, vocab, transform=None, max_samples=None):\n",
    "        self.root = root\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "        self.samples = []\n",
    "        with open(captions_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                # Skip header\n",
    "                if line.startswith(\"image_name\"):\n",
    "                    continue\n",
    "\n",
    "                # Split by '|'\n",
    "                parts = [p.strip() for p in line.split(\"|\")]\n",
    "                if len(parts) < 3:\n",
    "                    continue\n",
    "\n",
    "                img_name,_, caption = parts\n",
    "                self.samples.append((img_name, caption))\n",
    "            \n",
    "            if max_samples:\n",
    "                self.samples = self.samples[:max_samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vocab = self.vocab\n",
    "        img_name, caption = self.samples[index]\n",
    "        img_path = os.path.join(self.root, img_name)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except:\n",
    "            # Skip corrupted image\n",
    "            return self.__getitem__((index + 1) % len(self.image_ids))\n",
    "        \n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "\n",
    "        caption_indices = [vocab.word2idx[\"<start>\"]]\n",
    "        caption_indices += [\n",
    "            vocab.word2idx.get(token, vocab.word2idx[\"<unk>\"])\n",
    "            for token in tokens\n",
    "        ]\n",
    "        caption_indices.append(vocab.word2idx[\"<end>\"])\n",
    "\n",
    "        caption_tensor = torch.tensor(caption_indices)\n",
    "\n",
    "        return image, caption_tensor, img_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38723c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 47\u001b[0m\n\u001b[0;32m     38\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     39\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[0;32m     40\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39m collate_fn\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_dataset))\n\u001b[1;32m---> 47\u001b[0m image, caption, ids \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(image))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape)        \u001b[38;5;66;03m# after transform\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[36], line 49\u001b[0m, in \u001b[0;36mFlickrDataset30K.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m     48\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\n\u001b[1;32m---> 49\u001b[0m     img_name, caption \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     50\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, img_name)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, captions, ids = zip(*batch)\n",
    "\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    captions = pad_sequence(\n",
    "        captions,\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "\n",
    "    return images, captions,ids\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "# from flickr_dataset  import FlickrDataset \n",
    "\n",
    "test_dataset = FlickrDataset30K(\n",
    "    root=IMAGES_PATH,\n",
    "    captions_path=CAPTIONS_PATH,\n",
    "    vocab=vocab,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn= collate_fn\n",
    ")\n",
    "\n",
    "print(len(test_dataset))\n",
    "image, caption, ids = test_dataset[0]\n",
    "\n",
    "print(type(image))\n",
    "print(image.shape)        # after transform\n",
    "print(caption)\n",
    "print(len(caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798a4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import TransformerEncoderViT\n",
    "from model import TransformerDecoder\n",
    "\n",
    "vocab = torch.load(\"models/vocab.pkl\", weights_only=False)\n",
    "encoder = TransformerEncoderViT(256).to(device)\n",
    "decoder = TransformerDecoder(embed_size=256, vocab_size=len(vocab)).to(device)\n",
    "\n",
    "encoder.load_state_dict(torch.load(\"models/encoder.pth\", map_location=device))\n",
    "decoder.load_state_dict(torch.load(\"models/decoder.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def tokens_to_string(tokens):\n",
    "    if isinstance(tokens, list):\n",
    "        return \" \".join(tokens)\n",
    "    return tokens\n",
    "\n",
    "def compute_rouge_l(references, hypotheses):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for refs, hyp in zip(references, hypotheses):\n",
    "        hyp_str = tokens_to_string(hyp)\n",
    "\n",
    "        rouge_l_scores = []\n",
    "        for ref in refs:\n",
    "            ref_str = tokens_to_string(ref)\n",
    "            score = scorer.score(ref_str, hyp_str)['rougeL'].fmeasure\n",
    "            rouge_l_scores.append(score)\n",
    "\n",
    "        scores.append(max(rouge_l_scores))  # best reference\n",
    "\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "from jiwer import wer\n",
    "def compute_wer(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Computes WER using best matching reference per hypothesis\n",
    "    \"\"\"\n",
    "    wers = []\n",
    "    ref_tokens = []\n",
    "    for refs in references:\n",
    "        processed_refs = []\n",
    "        for ref in refs:\n",
    "            if isinstance(ref, list):\n",
    "                processed_refs.append(ref)        # already tokenized\n",
    "            else:\n",
    "                processed_refs.append(ref.split()) # string → tokens\n",
    "        ref_tokens.append(processed_refs)\n",
    "\n",
    "    # Handle hypotheses\n",
    "    hyp_tokens = []\n",
    "    for hyp in hypotheses:\n",
    "        if isinstance(hyp, list):\n",
    "            hyp_tokens.append(hyp)\n",
    "        else:\n",
    "            hyp_tokens.append(hyp.split())\n",
    "\n",
    "    for refs, hyp in zip(references, hypotheses):\n",
    "        ref_wers = [wer(ref, hyp) for ref in refs]\n",
    "        wers.append(min(ref_wers))  # Best match\n",
    "\n",
    "    return sum(wers) / len(wers)\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "def clean_caption(tokens, special_tokens={\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"}):\n",
    "    return [w for w in tokens if w not in special_tokens]\n",
    "\n",
    "def compute_bleu_scores(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    dataloader,\n",
    "    vocab,\n",
    "    device,\n",
    "    decode_fn,          # greedy or beam decode function\n",
    "    max_len=30\n",
    "):\n",
    "    \"\"\"\n",
    "    references: list of list of reference captions\n",
    "    hypotheses: list of predicted captions\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    references = []   # list of list of list of words\n",
    "    hypotheses = []   # list of list of words\n",
    "\n",
    "    for images, captions, ids in dataloader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            image = images[i]\n",
    "\n",
    "            # ---- generate caption ----\n",
    "            pred_sentence = decode_fn(\n",
    "                image=image,\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                vocab=vocab,\n",
    "                max_len=max_len\n",
    "            )\n",
    "\n",
    "            pred_tokens = clean_caption(pred_sentence.split())\n",
    "            hypotheses.append(pred_tokens)\n",
    "\n",
    "            # ---- SINGLE reference caption ----\n",
    "            cap_tokens = captions[i].tolist()  # [max_len]\n",
    "            ref_words = [\n",
    "                vocab.idx2word[idx]\n",
    "                for idx in cap_tokens\n",
    "                if idx not in (\n",
    "                    vocab.word2idx[\"<start>\"],\n",
    "                    vocab.word2idx[\"<end>\"],\n",
    "                    vocab.word2idx[\"<pad>\"],\n",
    "                    vocab.word2idx[\"<unk>\"]\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            references.append([ref_words])  # <-- note: list of list\n",
    "\n",
    "     # ---- BLEU scores ----\n",
    "    # bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
    "    # bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
    "    # bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "    return {\n",
    "        # \"BLEU-1\": bleu1,\n",
    "        # \"BLEU-2\": bleu2,\n",
    "        # \"BLEU-3\": bleu3,\n",
    "        \"BLEU-4\": bleu4\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb588e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode_transformer(image, encoder, decoder, vocab, max_len=30):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    bos = vocab.word2idx[\"<start>\"]\n",
    "    eos = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    image = image.unsqueeze(0)\n",
    "    memory = encoder(image)\n",
    "\n",
    "    generated = torch.tensor([[bos]], device=image.device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits = decoder(memory, generated)\n",
    "        next_token = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == eos:\n",
    "            break\n",
    "\n",
    "    words = []\n",
    "    for idx in generated.squeeze(0).tolist():\n",
    "        w = vocab.idx2word[idx]\n",
    "        if w in (\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"):\n",
    "            continue\n",
    "        words.append(w)\n",
    "\n",
    "    return \" \".join(words)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode_transformer(image, encoder, decoder, vocab, max_len=30):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    bos = vocab.word2idx[\"<start>\"]\n",
    "    eos = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    image = image.unsqueeze(0)\n",
    "    memory = encoder(image)\n",
    "\n",
    "    generated = torch.tensor([[bos]], device=image.device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits = decoder(memory, generated)\n",
    "        next_token = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == eos:\n",
    "            break\n",
    "\n",
    "    words = []\n",
    "    for idx in generated.squeeze(0).tolist():\n",
    "        w = vocab.idx2word[idx]\n",
    "        if w in (\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"):\n",
    "            continue\n",
    "        words.append(w)\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ca27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BLEU-4': 0.0539460363495956}\n"
     ]
    }
   ],
   "source": [
    "bleu_scores = compute_bleu_scores(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    dataloader=test_loader,\n",
    "    vocab=vocab,\n",
    "    device=device,\n",
    "    decode_fn=greedy_decode_transformer,\n",
    "    max_len=30\n",
    ")\n",
    "\n",
    "print(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def recall_at_k(similarity, k):\n",
    "    \"\"\"\n",
    "    similarity: [N, N] similarity matrix\n",
    "    \"\"\"\n",
    "    topk = similarity.topk(k, dim=1).indices\n",
    "    targets = torch.arange(similarity.size(0)).unsqueeze(1).to(similarity.device)\n",
    "    correct = (topk == targets).any(dim=1)\n",
    "    return correct.float().mean().item()\n",
    "\n",
    "def extract_image_embeddings(dataloader, encoder, device):\n",
    "    encoder.eval()\n",
    "    image_embeddings = []\n",
    "    all_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ , image_ids in dataloader:\n",
    "            images = images.to(device)\n",
    "            patch_emb = encoder(images)               # [B, 196, 256]\n",
    "            feats = patch_emb.mean(dim=1)            # [B, 256]\n",
    "            feats = F.normalize(feats, dim=1)\n",
    "            image_embeddings.append(feats)\n",
    "            all_ids.extend(image_ids) \n",
    "\n",
    "    return torch.cat(image_embeddings, dim=0) , all_ids      # [N, embed_size]\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_text_embeddings(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    dataloader,\n",
    "    vocab,\n",
    "    device\n",
    "):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_ids = []\n",
    "\n",
    "    pad_idx = vocab.word2idx[\"<pad>\"]\n",
    "\n",
    "    for images, captions, image_ids in dataloader:\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # ---- Encode images ----\n",
    "        memory = encoder(images)  # [B, N, D]\n",
    "\n",
    "        # ---- Transformer decoder forward (replicates decoder internals) ----\n",
    "        B, T = captions.shape\n",
    "        positions = torch.arange(T, device=device).unsqueeze(0)\n",
    "\n",
    "        x = decoder.embed(captions) + decoder.pos_embed(positions)\n",
    "        x = x * math.sqrt(decoder.embed_size)\n",
    "\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(T, T, device=device), diagonal=1\n",
    "        ).bool()\n",
    "\n",
    "        padding_mask = captions == pad_idx\n",
    "\n",
    "        out = decoder.decoder(\n",
    "            tgt=x,\n",
    "            memory=memory,\n",
    "            tgt_mask=causal_mask,\n",
    "            tgt_key_padding_mask=padding_mask\n",
    "        )  # [B, T, D]\n",
    "\n",
    "        # ---- extract last valid token per sentence ----\n",
    "        lengths = (~padding_mask).sum(dim=1) - 1\n",
    "        sent_emb = out[torch.arange(B), lengths]  # [B, D]\n",
    "\n",
    "        sent_emb = F.normalize(sent_emb, dim=1)\n",
    "\n",
    "        all_embeddings.append(sent_emb.cpu())\n",
    "        all_ids.extend(image_ids)\n",
    "\n",
    "    return torch.cat(all_embeddings, dim=0), all_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b0edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image_to_text_retrieval(image_emb, text_emb, image_ids, text_ids, batch_size=512):\n",
    "    image_emb = image_emb.cuda()\n",
    "    text_emb  = text_emb.cuda()\n",
    "\n",
    "     # build ID → index map for text embeddings\n",
    "    text_id_to_index = {tid: idx for idx, tid in enumerate(text_ids)}\n",
    "\n",
    "    N = image_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        img_batch = image_emb[i:i+batch_size]           # [B, D]\n",
    "        sim = img_batch @ text_emb.T                     # [B, N]\n",
    "\n",
    "        batch_image_ids = image_ids[i:i+batch_size]\n",
    "        \n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j, img_id in enumerate(batch_image_ids):\n",
    "            gt_index = text_id_to_index[img_id]\n",
    "\n",
    "            rank = (sorted_idx[j] == gt_index).nonzero(as_tuple=True)[0].item()\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = torch.tensor(ranks, device=image_emb.device)\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }\n",
    "\n",
    "def text_to_image_retrieval(image_emb, text_emb, image_ids, text_ids, batch_size=512):\n",
    "    text_emb  = text_emb.cuda()\n",
    "    image_emb = image_emb.cuda()\n",
    "\n",
    "    # build ID → index map for images\n",
    "    image_id_to_index = {iid: idx for idx, iid in enumerate(image_ids)}\n",
    "\n",
    "    N = text_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        txt_batch = text_emb[i:i+batch_size]     # [B, D]\n",
    "        sim = txt_batch @ image_emb.T             # [B, N_image]\n",
    "\n",
    "        batch_text_ids = text_ids[i:i+batch_size]\n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j, txt_id in enumerate(batch_text_ids):\n",
    "            gt_index = image_id_to_index[txt_id]  # ✅ correct GT image\n",
    "\n",
    "            rank = (sorted_idx[j] == gt_index).nonzero(as_tuple=True)[0].item()\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = torch.tensor(ranks, device=text_emb.device)\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }\n",
    "\n",
    "def text_to_text_retrieval(text_emb, text_ids, batch_size=512):\n",
    "    text_emb = text_emb.cuda()\n",
    "\n",
    "    # build image_id → list of text indices\n",
    "    id_to_indices = {}\n",
    "    for idx, img_id in enumerate(text_ids):\n",
    "        id_to_indices.setdefault(img_id, []).append(idx)\n",
    "\n",
    "    N = text_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        txt_batch = text_emb[i:i+batch_size]     # [B, D]\n",
    "        sim = txt_batch @ text_emb.T              # [B, N]\n",
    "\n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j in range(sorted_idx.size(0)):\n",
    "            query_idx = i + j\n",
    "            img_id = text_ids[query_idx]\n",
    "\n",
    "            # valid GT indices = same image_id, excluding itself\n",
    "            gt_indices = [idx for idx in id_to_indices[img_id] if idx != query_idx]\n",
    "\n",
    "            # find best (lowest) rank among all GT indices\n",
    "            rank = min(\n",
    "                (sorted_idx[j] == gt).nonzero(as_tuple=True)[0].item()\n",
    "                for gt in gt_indices\n",
    "            )\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = torch.tensor(ranks, device=text_emb.device)\n",
    "\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }\n",
    "\n",
    "def image_to_image_retrieval(image_emb, image_ids, batch_size=512):\n",
    "    \"\"\"\n",
    "    image_emb : Tensor [N, D]\n",
    "    image_ids : list of image_ids\n",
    "    \"\"\"\n",
    "\n",
    "    image_emb = image_emb.cuda()\n",
    "\n",
    "    # build image_id → list of indices\n",
    "    id_to_indices = {}\n",
    "    for idx, img_id in enumerate(image_ids):\n",
    "        id_to_indices.setdefault(img_id, []).append(idx)\n",
    "\n",
    "    N = image_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        img_batch = image_emb[i:i+batch_size]    # [B, D]\n",
    "        sim = img_batch @ image_emb.T             # [B, N]\n",
    "\n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j in range(sorted_idx.size(0)):\n",
    "            query_idx = i + j\n",
    "            img_id = image_ids[query_idx]\n",
    "\n",
    "            # same ID images excluding itself\n",
    "            gt_indices = [idx for idx in id_to_indices[img_id] if idx != query_idx]\n",
    "\n",
    "            if len(gt_indices) == 0:\n",
    "                continue  # only one image per ID → skip\n",
    "\n",
    "            rank = min(\n",
    "                (sorted_idx[j] == gt).nonzero(as_tuple=True)[0].item()\n",
    "                for gt in gt_indices\n",
    "            )\n",
    "            ranks.append(rank)\n",
    "\n",
    "    if len(ranks) == 0:\n",
    "        return {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0}\n",
    "\n",
    "    ranks = torch.tensor(ranks, device=image_emb.device)\n",
    "\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_caption_transformer(image, encoder, decoder, vocab, max_len=30):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    bos = vocab.word2idx[\"<start>\"]\n",
    "    eos = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    memory = encoder(image)  # [1, N, D]\n",
    "\n",
    "    generated = torch.tensor([[bos]], device=device)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        logits = decoder(memory, generated)      # [1, t, V]\n",
    "        next_token = logits[:, -1].argmax(dim=-1, keepdim=True)  # [1,1]\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == eos:\n",
    "            break\n",
    "\n",
    "    words = []\n",
    "    for idx in generated.squeeze(0).tolist():\n",
    "        w = vocab.idx2word[idx]\n",
    "        if w in (\"<start>\", \"<pad>\"):\n",
    "            continue\n",
    "        if w == \"<end>\":\n",
    "            break\n",
    "        words.append(w)\n",
    "\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_emb, image_ids  = extract_image_embeddings(test_loader, encoder, device)\n",
    "text_emb, text_ids = extract_text_embeddings(encoder, decoder, test_loader, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b7f8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image_ids: ['3509575615_653cbf01fc.jpg', '3509575615_653cbf01fc.jpg', '3509575615_653cbf01fc.jpg', '3509575615_653cbf01fc.jpg', '3509575615_653cbf01fc.jpg']\n",
      "Sample text_ids : ['3509575615_653cbf01fc.jpg', '3509575615_653cbf01fc.jpg', '3509575615_653cbf01fc.jpg', '3509575615_653cbf01fc.jpg', '3509575615_653cbf01fc.jpg']\n",
      "image_ids unique: 1000\n",
      "text_ids unique : 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample image_ids:\", image_ids[:5])\n",
    "print(\"Sample text_ids :\", text_ids[:5])\n",
    "print(\"image_ids unique:\", len(set(image_ids)))\n",
    "print(\"text_ids unique :\", len(set(text_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b8c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image → Text: {'R@1': 0.0, 'R@5': 0.0009999999310821295, 'R@10': 0.0009999999310821295}\n",
      "Text → Image: {'R@1': 0.0, 'R@5': 0.00019999999494757503, 'R@10': 0.0005999999702908099}\n",
      "Text → Text : {'R@1': 0.00019999999494757503, 'R@5': 0.988599956035614, 'R@10': 0.9945999979972839}\n",
      "Image → Image: {'R@1': 0.7999999523162842, 'R@5': 1.0, 'R@10': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Image → Text:\", image_to_text_retrieval(image_emb, text_emb, image_ids, text_ids))\n",
    "print(\"Text → Image:\", text_to_image_retrieval(image_emb, text_emb, image_ids, text_ids))\n",
    "print(\"Text → Text :\", text_to_text_retrieval(text_emb, text_ids))\n",
    "print(\"Image → Image:\", image_to_image_retrieval(image_emb, image_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
