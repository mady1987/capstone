{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad43108-8fac-410d-9930-0912134d8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import clip\n",
    "import open_clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a610026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE RUN THE download_datasets.ipynb PRESENT IN THE BASE DIRECTORY OF THE REPO FIRST TO DOWNLOAD THE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = \"..\\\\..\\\\datasets\\\\\"\n",
    "models = \"..\\\\..\\\\models\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb8562",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7788bf-4e58-4ef0-9ab6-ca466ea2897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/16\", device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751498e0-408e-4ae0-b004-2c53a86db696",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = f\"{datasets}/flickr8k/Images\"        # Folder containing 8000 images\n",
    "CAPTIONS_PATH = f\"{datasets}/flickr8k/captions.txt\"  # Caption file\n",
    "MODEL_PATH = f\"{models}\"  # Path to save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d77fe-f160-4156-acd9-e9b18273a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "captions = []\n",
    "\n",
    "with open(CAPTIONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # skip header: image,caption\n",
    "\n",
    "    for row in reader:\n",
    "        if len(row) < 2:\n",
    "            continue\n",
    "        img_name, caption = row\n",
    "        captions.append((img_name.strip(), caption.strip()))\n",
    "\n",
    "print(\"Total captions:\", len(captions))\n",
    "print(\"Sample:\", captions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecaad52-5aaa-42eb-8b72-e849250fb5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = {}\n",
    "\n",
    "print(\"Extracting image embeddings...\")\n",
    "for img_name in tqdm(os.listdir(IMAGES_PATH)):\n",
    "    img_path = os.path.join(IMAGES_PATH, img_name)\n",
    "\n",
    "    try:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = model.encode_image(image_input)\n",
    "        emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    image_features[img_name] = emb.cpu()\n",
    "\n",
    "torch.save(image_features, f\"{models}/image_features_flickr8k.pt\")\n",
    "print(\"Saved image features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad6cd90-d742-485a-82ad-de079ba570ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_features = {}\n",
    "\n",
    "print(\"Extracting caption embeddings with ensembling...\")\n",
    "\n",
    "# Step 1: group captions by image\n",
    "from collections import defaultdict\n",
    "captions_by_image = defaultdict(list)\n",
    "for img_name, caption in captions:\n",
    "    captions_by_image[img_name].append(caption)\n",
    "\n",
    "for img_name, caps in tqdm(captions_by_image.items()):\n",
    "    emb_list = []\n",
    "    for cap in caps:\n",
    "        text_input = clip.tokenize([cap]).to(device)\n",
    "        with torch.no_grad():\n",
    "            emb = model.encode_text(text_input)\n",
    "            emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "            emb_list.append(emb.cpu())\n",
    "    # -----------------------------\n",
    "    # Caption ensembling: average embeddings of all 5 captions\n",
    "    # -----------------------------\n",
    "    caption_features[img_name] = torch.stack(emb_list).mean(dim=0)\n",
    "    caption_features[img_name] /= caption_features[img_name].norm()  # normalize\n",
    "\n",
    "torch.save(caption_features, f\"{models}/caption_features_flickr8k.pt\")\n",
    "print(\"Saved caption features with ensembling!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
