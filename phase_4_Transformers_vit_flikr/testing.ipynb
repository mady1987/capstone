{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b68b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d96a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGES_PATH = \"../phase_1/data/flickr8k/images/\"  # Directory with training images\n",
    "# CAPTIONS_PATH = \"../phase_1/data/flickr8k/captions_testImages.txt\"  # Caption file\n",
    "# TEST_IMAGES_PATH = \"../phase_1/test copy/\"  # Directory with test images\n",
    "\n",
    "datasets = \"..\\\\..\\\\datasets\\\\\"\n",
    "models = \"..\\\\..\\\\models\\\\\"\n",
    "caption_model = torch.load(f\"{models}/caption_features_flickr8k.pt\")\n",
    "\n",
    "IMAGES_PATH = f\"{datasets}/flickr8k/images\"  # Directory with training images\n",
    "CAPTIONS_PATH = f\"{datasets}/flickr8k/captions.txt\"  # Caption file\n",
    "TEST_IMAGES_PATH = \"..\\\\test_images\"  # Directory with test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbfa33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import nltk \n",
    "from collections import Counter\n",
    "from vocabulary_class  import Vocabulary\n",
    "nltk.download('punkt_tab')\n",
    "import json\n",
    "import csv\n",
    "\n",
    "tokens = []\n",
    "counter = Counter()\n",
    "\n",
    "def build_vocab(json_path, threshold=5, limit=None):\n",
    "    \n",
    "    counter = Counter()\n",
    "    image_captions = {}\n",
    "    count =0\n",
    "    with open(CAPTIONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # skip header: image,caption\n",
    "\n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue\n",
    "            img_name, caption = row\n",
    "            if img_name not in image_captions:\n",
    "                    image_captions[img_name] = []\n",
    "            image_captions[img_name].append(caption)\n",
    "\n",
    "            caption = caption.lower()\n",
    "            tokens = nltk.tokenize.word_tokenize(caption)\n",
    "            counter.update(tokens)\n",
    "            count +=1\n",
    "            if limit and count >= limit:\n",
    "                break\n",
    "    \n",
    "    vocab = Vocabulary()\n",
    "    for word, cnt in counter.items():\n",
    "        if cnt >= threshold:\n",
    "            vocab.add_word(word)\n",
    "    \n",
    "    return vocab, image_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9695df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, image_captions = build_vocab(CAPTIONS_PATH, threshold=5)\n",
    "print(\"Total vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82246f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Update the folowing datset call class to handle flikr8k dataset formate\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root, captions_path, vocab, transform=None, max_samples=None):\n",
    "        self.root = root\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "        self.samples = []  # (img_name, caption)\n",
    "\n",
    "        with open(captions_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for img_name, caption in reader:\n",
    "                self.samples.append((img_name, caption))\n",
    "\n",
    "            if max_samples:\n",
    "                self.samples = self.samples[:max_samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name, caption = self.samples[index]\n",
    "        img_path = os.path.join(self.root, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        tokens = nltk.tokenize.word_tokenize(caption)\n",
    "        caption_indices = [self.vocab.word2idx[\"<start>\"]] + \\\n",
    "            [self.vocab.word2idx.get(t, self.vocab.word2idx[\"<unk>\"]) for t in tokens] + \\\n",
    "            [self.vocab.word2idx[\"<end>\"]]\n",
    "\n",
    "        caption_tensor = torch.tensor(caption_indices)\n",
    "\n",
    "        return image, caption_tensor, img_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38723c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, captions, ids = zip(*batch)\n",
    "\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    captions = pad_sequence(\n",
    "        captions,\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "\n",
    "    return images, captions,ids\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "# from flickr_dataset  import FlickrDataset \n",
    "\n",
    "test_dataset = FlickrDataset(\n",
    "    root=IMAGES_PATH,\n",
    "    captions_path=CAPTIONS_PATH,\n",
    "    vocab=vocab,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn= collate_fn\n",
    ")\n",
    "\n",
    "print(len(test_dataset))\n",
    "image, caption, ids = test_dataset[0]\n",
    "\n",
    "print(type(image))\n",
    "print(image.shape)        # after transform\n",
    "print(caption)\n",
    "print(len(caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TransformerEncoderViT\n",
    "from model import TransformerCaptionDecoder\n",
    "\n",
    "vocab = torch.load(f\"{models}/vocab-VT.pkl\", weights_only=False)\n",
    "encoder = TransformerEncoderViT().to(device)\n",
    "decoder = TransformerCaptionDecoder(embed_size=256, vocab_size=len(vocab)).to(device)\n",
    "\n",
    "encoder.load_state_dict(torch.load(f\"{models}/encoder-Vit-Transformer.pth\", map_location=device))\n",
    "decoder.load_state_dict(torch.load(f\"{models}/decoder-VT.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def tokens_to_string(tokens):\n",
    "    if isinstance(tokens, list):\n",
    "        return \" \".join(tokens)\n",
    "    return tokens\n",
    "\n",
    "def compute_rouge_l(references, hypotheses):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for refs, hyp in zip(references, hypotheses):\n",
    "        hyp_str = tokens_to_string(hyp)\n",
    "\n",
    "        rouge_l_scores = []\n",
    "        for ref in refs:\n",
    "            ref_str = tokens_to_string(ref)\n",
    "            score = scorer.score(ref_str, hyp_str)['rougeL'].fmeasure\n",
    "            rouge_l_scores.append(score)\n",
    "\n",
    "        scores.append(max(rouge_l_scores))  # best reference\n",
    "\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "from jiwer import wer\n",
    "def compute_wer(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Computes WER using best matching reference per hypothesis\n",
    "    \"\"\"\n",
    "    wers = []\n",
    "    ref_tokens = []\n",
    "    for refs in references:\n",
    "        processed_refs = []\n",
    "        for ref in refs:\n",
    "            if isinstance(ref, list):\n",
    "                processed_refs.append(ref)        # already tokenized\n",
    "            else:\n",
    "                processed_refs.append(ref.split()) # string → tokens\n",
    "        ref_tokens.append(processed_refs)\n",
    "\n",
    "    # Handle hypotheses\n",
    "    hyp_tokens = []\n",
    "    for hyp in hypotheses:\n",
    "        if isinstance(hyp, list):\n",
    "            hyp_tokens.append(hyp)\n",
    "        else:\n",
    "            hyp_tokens.append(hyp.split())\n",
    "\n",
    "    for refs, hyp in zip(references, hypotheses):\n",
    "        ref_wers = [wer(ref, hyp) for ref in refs]\n",
    "        wers.append(min(ref_wers))  # Best match\n",
    "\n",
    "    return sum(wers) / len(wers)\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "def clean_caption(tokens, special_tokens={\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"}):\n",
    "    return [w for w in tokens if w not in special_tokens]\n",
    "\n",
    "def compute_bleu_scores(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    dataloader,\n",
    "    vocab,\n",
    "    device,\n",
    "    decode_fn,          # greedy or beam decode function\n",
    "    max_len=30\n",
    "):\n",
    "    \"\"\"\n",
    "    references: list of list of reference captions\n",
    "    hypotheses: list of predicted captions\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    references = []   # list of list of list of words\n",
    "    hypotheses = []   # list of list of words\n",
    "\n",
    "    for images, captions, ids in dataloader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            image = images[i]\n",
    "\n",
    "            # ---- generate caption ----\n",
    "            pred_sentence = decode_fn(\n",
    "                image=image,\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                vocab=vocab,\n",
    "                max_len=max_len\n",
    "            )\n",
    "\n",
    "            pred_tokens = clean_caption(pred_sentence.split())\n",
    "            hypotheses.append(pred_tokens)\n",
    "\n",
    "            # ---- SINGLE reference caption ----\n",
    "            cap_tokens = captions[i].tolist()  # [max_len]\n",
    "            ref_words = [\n",
    "                vocab.idx2word[idx]\n",
    "                for idx in cap_tokens\n",
    "                if idx not in (\n",
    "                    vocab.word2idx[\"<start>\"],\n",
    "                    vocab.word2idx[\"<end>\"],\n",
    "                    vocab.word2idx[\"<pad>\"],\n",
    "                    vocab.word2idx[\"<unk>\"]\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            references.append([\n",
    "                clean_caption(nltk.word_tokenize(c.lower()))\n",
    "                for c in image_captions[ids[i]]\n",
    "                ])\n",
    "            references.append([ref_words])  # <-- note: list of list\n",
    "\n",
    "     # ---- BLEU scores ----\n",
    "    # bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
    "    # bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
    "    # bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "    return {\n",
    "        # \"BLEU-1\": bleu1,\n",
    "        # \"BLEU-2\": bleu2,\n",
    "        # \"BLEU-3\": bleu3,\n",
    "        \"BLEU-4\": bleu4\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb588e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode_transformer(image, encoder, decoder, vocab, max_len=30):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    bos = vocab.word2idx[\"<start>\"]\n",
    "    eos = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    image = image.unsqueeze(0)\n",
    "    memory = encoder(image)\n",
    "\n",
    "    generated = torch.tensor([[bos]], device=image.device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits = decoder(memory, generated)\n",
    "        next_token = logits[:, -1, vocab.word2idx[\"<unk>\"]] = -1e9\n",
    "\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == eos:\n",
    "            break\n",
    "\n",
    "    words = []\n",
    "    for idx in generated.squeeze(0).tolist():\n",
    "        w = vocab.idx2word[idx]\n",
    "        if w in (\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"):\n",
    "            continue\n",
    "        words.append(w)\n",
    "\n",
    "    return \" \".join(words)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ca27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores = compute_bleu_scores(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    dataloader=test_loader,\n",
    "    vocab=vocab,\n",
    "    device=device,\n",
    "    decode_fn=greedy_decode_transformer,\n",
    "    max_len=30\n",
    ")\n",
    "\n",
    "print(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def recall_at_k(similarity, k):\n",
    "    \"\"\"\n",
    "    similarity: [N, N] similarity matrix\n",
    "    \"\"\"\n",
    "    topk = similarity.topk(k, dim=1).indices\n",
    "    targets = torch.arange(similarity.size(0)).unsqueeze(1).to(similarity.device)\n",
    "    correct = (topk == targets).any(dim=1)\n",
    "    return correct.float().mean().item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_image_embeddings(loader, encoder, decoder, device):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    all_embs = []\n",
    "    all_ids = []\n",
    "\n",
    "    for images, _, image_ids in tqdm.tqdm(loader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        enc_out = encoder(images)              # [B, 196, 768]\n",
    "        enc_out = decoder.enc_proj(enc_out)    # ✅ project to 256\n",
    "\n",
    "        img_emb = enc_out.mean(dim=1)          # [B, 256]\n",
    "        img_emb = F.normalize(img_emb, dim=1)\n",
    "\n",
    "        all_embs.append(img_emb.cpu())\n",
    "        all_ids.extend(image_ids)\n",
    "\n",
    "    return torch.cat(all_embs, dim=0), all_ids\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_text_embeddings(encoder, decoder, loader, vocab, device):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    all_embs = []\n",
    "    all_ids = []\n",
    "\n",
    "    pad_idx = vocab.word2idx[\"<pad>\"]\n",
    "\n",
    "    for images, captions, image_ids in tqdm.tqdm(loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # ---- Encoder ----\n",
    "        encoder_out = encoder(images)  # [B, 196, 768]\n",
    "        memory = decoder.enc_proj(encoder_out)  # [B, 196, D]\n",
    "\n",
    "        # ---- Decoder embeddings (MATCH TRAINING) ----\n",
    "        tgt = decoder.embed(captions) * math.sqrt(decoder.embed.embedding_dim)\n",
    "        tgt = decoder.pos(tgt)\n",
    "\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
    "            captions.size(1)\n",
    "        ).to(device)\n",
    "\n",
    "        padding_mask = captions == pad_idx\n",
    "\n",
    "        hidden = decoder.decoder(\n",
    "            tgt=tgt,\n",
    "            memory=memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=padding_mask\n",
    "        )  # [B, T, D]\n",
    "\n",
    "        # ---- last non-pad token ----\n",
    "        lengths = (~padding_mask).sum(dim=1) - 1\n",
    "        sent_emb = hidden[torch.arange(hidden.size(0)), lengths]\n",
    "\n",
    "        sent_emb = F.normalize(sent_emb, dim=1)\n",
    "\n",
    "        all_embs.append(sent_emb.cpu())\n",
    "        all_ids.extend(image_ids)\n",
    "\n",
    "    return torch.cat(all_embs, dim=0), all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b0edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image_to_text_retrieval(image_emb, text_emb, image_ids, text_ids, batch_size=512):\n",
    "    image_emb = image_emb.cuda()\n",
    "    text_emb  = text_emb.cuda()\n",
    "\n",
    "     # build ID → index map for text embeddings\n",
    "    text_id_to_index = {tid: idx for idx, tid in enumerate(text_ids)}\n",
    "\n",
    "    N = image_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        img_batch = image_emb[i:i+batch_size]           # [B, D]\n",
    "        sim = img_batch @ text_emb.T                     # [B, N]\n",
    "\n",
    "        batch_image_ids = image_ids[i:i+batch_size]\n",
    "        \n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j, img_id in enumerate(batch_image_ids):\n",
    "            gt_index = text_id_to_index[img_id]\n",
    "\n",
    "            rank = (sorted_idx[j] == gt_index).nonzero(as_tuple=True)[0].item()\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = torch.tensor(ranks, device=image_emb.device)\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }\n",
    "\n",
    "def text_to_image_retrieval(image_emb, text_emb, image_ids, text_ids, batch_size=512):\n",
    "    text_emb  = text_emb.cuda()\n",
    "    image_emb = image_emb.cuda()\n",
    "\n",
    "    # build ID → index map for images\n",
    "    image_id_to_index = {iid: idx for idx, iid in enumerate(image_ids)}\n",
    "\n",
    "    N = text_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        txt_batch = text_emb[i:i+batch_size]     # [B, D]\n",
    "        sim = txt_batch @ image_emb.T             # [B, N_image]\n",
    "\n",
    "        batch_text_ids = text_ids[i:i+batch_size]\n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j, txt_id in enumerate(batch_text_ids):\n",
    "            gt_index = image_id_to_index[txt_id]  # ✅ correct GT image\n",
    "\n",
    "            rank = (sorted_idx[j] == gt_index).nonzero(as_tuple=True)[0].item()\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = torch.tensor(ranks, device=text_emb.device)\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }\n",
    "\n",
    "def text_to_text_retrieval(text_emb, text_ids, batch_size=512):\n",
    "    text_emb = text_emb.cuda()\n",
    "\n",
    "    # build image_id → list of text indices\n",
    "    id_to_indices = {}\n",
    "    for idx, img_id in enumerate(text_ids):\n",
    "        id_to_indices.setdefault(img_id, []).append(idx)\n",
    "\n",
    "    N = text_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        txt_batch = text_emb[i:i+batch_size]     # [B, D]\n",
    "        sim = txt_batch @ text_emb.T              # [B, N]\n",
    "\n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j in range(sorted_idx.size(0)):\n",
    "            query_idx = i + j\n",
    "            img_id = text_ids[query_idx]\n",
    "\n",
    "            # valid GT indices = same image_id, excluding itself\n",
    "            gt_indices = [idx for idx in id_to_indices[img_id] if idx != query_idx]\n",
    "\n",
    "            if len(gt_indices) == 0:\n",
    "                continue  # only one caption → skip\n",
    "\n",
    "            rank = min(\n",
    "                (sorted_idx[j] == gt).nonzero(as_tuple=True)[0].item()\n",
    "                for gt in gt_indices\n",
    "            )\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = torch.tensor(ranks, device=text_emb.device)\n",
    "\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }\n",
    "\n",
    "def image_to_image_retrieval(image_emb, image_ids, batch_size=512):\n",
    "    \"\"\"\n",
    "    image_emb : Tensor [N, D]\n",
    "    image_ids : list of image_ids\n",
    "    \"\"\"\n",
    "\n",
    "    image_emb = image_emb.cuda()\n",
    "\n",
    "    # build image_id → list of indices\n",
    "    id_to_indices = {}\n",
    "    for idx, img_id in enumerate(image_ids):\n",
    "        id_to_indices.setdefault(img_id, []).append(idx)\n",
    "\n",
    "    N = image_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        img_batch = image_emb[i:i+batch_size]    # [B, D]\n",
    "        sim = img_batch @ image_emb.T             # [B, N]\n",
    "\n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j in range(sorted_idx.size(0)):\n",
    "            query_idx = i + j\n",
    "            img_id = image_ids[query_idx]\n",
    "\n",
    "            # same ID images excluding itself\n",
    "            gt_indices = [idx for idx in id_to_indices[img_id] if idx != query_idx]\n",
    "\n",
    "            if len(gt_indices) == 0:\n",
    "                continue  # only one image per ID → skip\n",
    "\n",
    "            rank = min(\n",
    "                (sorted_idx[j] == gt).nonzero(as_tuple=True)[0].item()\n",
    "                for gt in gt_indices\n",
    "            )\n",
    "            ranks.append(rank)\n",
    "\n",
    "    if len(ranks) == 0:\n",
    "        return {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0}\n",
    "\n",
    "    ranks = torch.tensor(ranks, device=image_emb.device)\n",
    "\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_caption_transformer(image, encoder, decoder, vocab, max_len=30):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    bos = vocab.word2idx[\"<start>\"]\n",
    "    eos = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    memory = encoder(image)  # [1, N, D]\n",
    "\n",
    "    generated = torch.tensor([[bos]], device=device)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        logits = decoder(memory, generated)      # [1, t, V]\n",
    "        next_token = logits[:, -1].argmax(dim=-1, keepdim=True)  # [1,1]\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == eos:\n",
    "            break\n",
    "\n",
    "    words = []\n",
    "    for idx in generated.squeeze(0).tolist():\n",
    "        w = vocab.idx2word[idx]\n",
    "        if w in (\"<start>\", \"<pad>\"):\n",
    "            continue\n",
    "        if w == \"<end>\":\n",
    "            break\n",
    "        words.append(w)\n",
    "\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_emb, image_ids  = extract_image_embeddings(test_loader, encoder, decoder, device)\n",
    "text_emb, text_ids = extract_text_embeddings(encoder, decoder, test_loader, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b7f8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample image_ids:\", image_ids[:5])\n",
    "print(\"Sample text_ids :\", text_ids[:5])\n",
    "print(\"image_ids unique:\", len(set(image_ids)))\n",
    "print(\"text_ids unique :\", len(set(text_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image → Text:\", image_to_text_retrieval(image_emb, text_emb, image_ids, text_ids))\n",
    "print(\"Text → Image:\", text_to_image_retrieval(image_emb, text_emb, image_ids, text_ids))\n",
    "print(\"Text → Text :\", text_to_text_retrieval(text_emb, text_ids))\n",
    "print(\"Image → Image:\", image_to_image_retrieval(image_emb, image_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
