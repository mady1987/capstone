{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12b68b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38723c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    captions = pad_sequence(\n",
    "        captions,\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "\n",
    "    return images, captions\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from flickr_dataset  import FlickrDataset \n",
    "\n",
    "train_dataset = FlickrDataset(\n",
    "    root=IMAGES_PATH,\n",
    "    captions_path=CAPTIONS_PATH,\n",
    "    vocab=vocab,\n",
    "    transform=transform,\n",
    "    max_samples=None\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn= collate_fn\n",
    ")\n",
    "\n",
    "print(len(train_dataset))\n",
    "image, caption = train_dataset[0]\n",
    "\n",
    "print(type(image))\n",
    "print(image.shape)        # after transform\n",
    "print(caption)\n",
    "print(len(caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee5d6850",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = \"../phase_1/data/flickr8k/images\"  # Directory with training images\n",
    "CAPTIONS_PATH = \"../phase_1/data/flickr8k/captions.txt\"  # Caption file\n",
    "TEST_IMAGES_PATH = \"../phase_1/test copy/\"  # Directory with test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a798a4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import TransformerEncoderViT\n",
    "from model import TransformerDecoder\n",
    "\n",
    "vocab = torch.load(\"models/vocab.pkl\", weights_only=False)\n",
    "encoder = TransformerEncoderViT(256).to(device)\n",
    "decoder = TransformerDecoder(embed_size=256, vocab_size=len(vocab)).to(device)\n",
    "\n",
    "encoder.load_state_dict(torch.load(\"models/encoder.pth\", map_location=device))\n",
    "decoder.load_state_dict(torch.load(\"models/decoder.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a4bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def tokens_to_string(tokens):\n",
    "    if isinstance(tokens, list):\n",
    "        return \" \".join(tokens)\n",
    "    return tokens\n",
    "\n",
    "def compute_rouge_l(references, hypotheses):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for refs, hyp in zip(references, hypotheses):\n",
    "        hyp_str = tokens_to_string(hyp)\n",
    "\n",
    "        rouge_l_scores = []\n",
    "        for ref in refs:\n",
    "            ref_str = tokens_to_string(ref)\n",
    "            score = scorer.score(ref_str, hyp_str)['rougeL'].fmeasure\n",
    "            rouge_l_scores.append(score)\n",
    "\n",
    "        scores.append(max(rouge_l_scores))  # best reference\n",
    "\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "from jiwer import wer\n",
    "def compute_wer(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Computes WER using best matching reference per hypothesis\n",
    "    \"\"\"\n",
    "    wers = []\n",
    "    ref_tokens = []\n",
    "    for refs in references:\n",
    "        processed_refs = []\n",
    "        for ref in refs:\n",
    "            if isinstance(ref, list):\n",
    "                processed_refs.append(ref)        # already tokenized\n",
    "            else:\n",
    "                processed_refs.append(ref.split()) # string â†’ tokens\n",
    "        ref_tokens.append(processed_refs)\n",
    "\n",
    "    # Handle hypotheses\n",
    "    hyp_tokens = []\n",
    "    for hyp in hypotheses:\n",
    "        if isinstance(hyp, list):\n",
    "            hyp_tokens.append(hyp)\n",
    "        else:\n",
    "            hyp_tokens.append(hyp.split())\n",
    "\n",
    "    for refs, hyp in zip(references, hypotheses):\n",
    "        ref_wers = [wer(ref, hyp) for ref in refs]\n",
    "        wers.append(min(ref_wers))  # Best match\n",
    "\n",
    "    return sum(wers) / len(wers)\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "def clean_caption(tokens, special_tokens={\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"}):\n",
    "    return [w for w in tokens if w not in special_tokens]\n",
    "\n",
    "def compute_bleu_scores(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    dataloader,\n",
    "    vocab,\n",
    "    device,\n",
    "    decode_fn,          # greedy or beam decode function\n",
    "    max_len=30\n",
    "):\n",
    "    \"\"\"\n",
    "    references: list of list of reference captions\n",
    "    hypotheses: list of predicted captions\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    references = []   # list of list of list of words\n",
    "    hypotheses = []   # list of list of words\n",
    "\n",
    "    for images, captions in dataloader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            image = images[i]\n",
    "\n",
    "            # ---- generate caption ----\n",
    "            pred_sentence = decode_fn(\n",
    "                image=image,\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                vocab=vocab,\n",
    "                max_len=max_len\n",
    "            )\n",
    "\n",
    "            pred_tokens = clean_caption(pred_sentence.split())\n",
    "            hypotheses.append(pred_tokens)\n",
    "\n",
    "            # ---- SINGLE reference caption ----\n",
    "            cap_tokens = captions[i].tolist()  # [max_len]\n",
    "            ref_words = [\n",
    "                vocab.idx2word[idx]\n",
    "                for idx in cap_tokens\n",
    "                if idx not in (\n",
    "                    vocab.word2idx[\"<start>\"],\n",
    "                    vocab.word2idx[\"<end>\"],\n",
    "                    vocab.word2idx[\"<pad>\"],\n",
    "                    vocab.word2idx[\"<unk>\"]\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            references.append([ref_words])  # <-- note: list of list\n",
    "\n",
    "     # ---- BLEU scores ----\n",
    "    # bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
    "    # bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
    "    # bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "    return {\n",
    "        # \"BLEU-1\": bleu1,\n",
    "        # \"BLEU-2\": bleu2,\n",
    "        # \"BLEU-3\": bleu3,\n",
    "        \"BLEU-4\": bleu4\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceb588e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode_transformer(image, encoder, decoder, vocab, max_len=30):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    bos = vocab.word2idx[\"<start>\"]\n",
    "    eos = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    image = image.unsqueeze(0)\n",
    "    memory = encoder(image)\n",
    "\n",
    "    generated = torch.tensor([[bos]], device=image.device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits = decoder(memory, generated)\n",
    "        next_token = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == eos:\n",
    "            break\n",
    "\n",
    "    words = []\n",
    "    for idx in generated.squeeze(0).tolist():\n",
    "        w = vocab.idx2word[idx]\n",
    "        if w in (\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"):\n",
    "            continue\n",
    "        words.append(w)\n",
    "\n",
    "    return \" \".join(words)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "972ca27d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m bleu_scores \u001b[38;5;241m=\u001b[39m compute_bleu_scores(\n\u001b[0;32m      2\u001b[0m     encoder\u001b[38;5;241m=\u001b[39mencoder,\n\u001b[0;32m      3\u001b[0m     decoder\u001b[38;5;241m=\u001b[39mdecoder,\n\u001b[1;32m----> 4\u001b[0m     dataloader\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_loader\u001b[49m,\n\u001b[0;32m      5\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m      6\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m      7\u001b[0m     decode_fn\u001b[38;5;241m=\u001b[39mgreedy_decode_transformer,\n\u001b[0;32m      8\u001b[0m     max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(bleu_scores)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "bleu_scores = compute_bleu_scores(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    dataloader=train_loader,\n",
    "    vocab=vocab,\n",
    "    device=device,\n",
    "    decode_fn=greedy_decode_transformer,\n",
    "    max_len=30\n",
    ")\n",
    "\n",
    "print(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def recall_at_k(similarity, k):\n",
    "    \"\"\"\n",
    "    similarity: [N, N] similarity matrix\n",
    "    \"\"\"\n",
    "    topk = similarity.topk(k, dim=1).indices\n",
    "    targets = torch.arange(similarity.size(0)).unsqueeze(1).to(similarity.device)\n",
    "    correct = (topk == targets).any(dim=1)\n",
    "    return correct.float().mean().item()\n",
    "\n",
    "def extract_image_embeddings(dataloader, encoder, device):\n",
    "    encoder.eval()\n",
    "    image_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _  in dataloader:\n",
    "            images = images.to(device)\n",
    "            patch_emb = encoder(images)               # [B, 196, 256]\n",
    "            feats = patch_emb.mean(dim=1)            # [B, 256]\n",
    "            feats = F.normalize(feats, dim=1)\n",
    "            image_embeddings.append(feats)\n",
    "\n",
    "    return torch.cat(image_embeddings, dim=0)       # [N, embed_size]\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_text_embeddings(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    dataloader,\n",
    "    vocab,\n",
    "    device\n",
    "):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_ids = []\n",
    "\n",
    "    pad_idx = vocab.word2idx[\"<pad>\"]\n",
    "\n",
    "    for images, captions in dataloader:\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # ---- Encode images ----\n",
    "        memory = encoder(images)  # [B, N, D]\n",
    "\n",
    "        # ---- Transformer decoder forward (replicates decoder internals) ----\n",
    "        B, T = captions.shape\n",
    "        positions = torch.arange(T, device=device).unsqueeze(0)\n",
    "\n",
    "        x = decoder.embed(captions) + decoder.pos_embed(positions)\n",
    "        x = x * math.sqrt(decoder.embed_size)\n",
    "\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(T, T, device=device), diagonal=1\n",
    "        ).bool()\n",
    "\n",
    "        padding_mask = captions == pad_idx\n",
    "\n",
    "        out = decoder.decoder(\n",
    "            tgt=x,\n",
    "            memory=memory,\n",
    "            tgt_mask=causal_mask,\n",
    "            tgt_key_padding_mask=padding_mask\n",
    "        )  # [B, T, D]\n",
    "\n",
    "        # ---- extract last valid token per sentence ----\n",
    "        lengths = (~padding_mask).sum(dim=1) - 1\n",
    "        sent_emb = out[torch.arange(B), lengths]  # [B, D]\n",
    "\n",
    "        sent_emb = F.normalize(sent_emb, dim=1)\n",
    "\n",
    "        all_embeddings.append(sent_emb.cpu())\n",
    "        all_ids.extend(range(len(sent_emb)))\n",
    "\n",
    "    return torch.cat(all_embeddings, dim=0), all_ids\n",
    "\n",
    "def image_to_text_retrieval(image_emb, text_emb, batch_size=512):\n",
    "    image_emb = image_emb.cuda()\n",
    "    text_emb  = text_emb.cuda()\n",
    "\n",
    "    N = image_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        img_batch = image_emb[i:i+batch_size]           # [B, D]\n",
    "        sim = img_batch @ text_emb.T                     # [B, N]\n",
    "\n",
    "        gt = torch.arange(i, min(i+batch_size, N)).cuda()\n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j in range(sorted_idx.size(0)):\n",
    "            rank = (sorted_idx[j] == gt[j]).nonzero(as_tuple=True)[0].item()\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = torch.tensor(ranks)\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }\n",
    "\n",
    "def text_to_image_retrieval(image_emb, text_emb, batch_size=512):\n",
    "    image_emb = image_emb.cuda()\n",
    "    text_emb  = text_emb.cuda()\n",
    "\n",
    "    N = text_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        txt_batch = text_emb[i:i+batch_size]           # [B, D]\n",
    "        sim = txt_batch @ image_emb.T                     # [B, N]\n",
    "\n",
    "        gt = torch.arange(i, min(i+batch_size, N)).cuda()\n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j in range(sorted_idx.size(0)):\n",
    "            rank = (sorted_idx[j] == gt[j]).nonzero(as_tuple=True)[0].item()\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = torch.tensor(ranks)\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }\n",
    "\n",
    "def text_to_text_retrieval(text_emb, batch_size=512):\n",
    "    text_emb  = text_emb.cuda()\n",
    "\n",
    "    N = text_emb.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        txt_batch = text_emb[i:i+batch_size]           # [B, D]\n",
    "        sim = txt_batch @ text_emb.T                     # [B, N]\n",
    "        sim.fill_diagonal_(-1)\n",
    "\n",
    "        gt = torch.arange(i, min(i+batch_size, N)).cuda()\n",
    "        sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "        for j in range(sorted_idx.size(0)):\n",
    "            rank = (sorted_idx[j] == gt[j]).nonzero(as_tuple=True)[0].item()\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = torch.tensor(ranks)\n",
    "    return {\n",
    "        \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "        \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "        \"R@10\": (ranks < 10).float().mean().item()\n",
    "    }\n",
    "\n",
    "# def image_to_image_retrieval(image_emb, batch_size=512):\n",
    "#     image_emb  = image_emb.cuda()\n",
    "\n",
    "#     N = image_emb.size(0)\n",
    "#     ranks = []\n",
    "\n",
    "#     for i in range(0, N, batch_size):\n",
    "#         img_batch = image_emb[i:i+batch_size]           # [B, D]\n",
    "#         sim = img_batch @ image_emb.T                     # [B, N]\n",
    "#         sim.fill_diagonal_(-1)\n",
    "\n",
    "#         gt = torch.arange(i, min(i+batch_size, N)).cuda()\n",
    "#         sorted_idx = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "#         for j in range(sorted_idx.size(0)):\n",
    "#             rank = (sorted_idx[j] == gt[j]).nonzero(as_tuple=True)[0].item()\n",
    "#             ranks.append(rank)\n",
    "\n",
    "#     ranks = torch.tensor(ranks)\n",
    "#     return {\n",
    "#         \"R@1\":  (ranks < 1).float().mean().item(),\n",
    "#         \"R@5\":  (ranks < 5).float().mean().item(),\n",
    "#         \"R@10\": (ranks < 10).float().mean().item()\n",
    "#     }\n",
    "\n",
    "def image_to_image_retrieval(image_emb):\n",
    "    sim = image_emb @ image_emb.t()\n",
    "\n",
    "    # Remove self-matching\n",
    "    sim.fill_diagonal_(-1)\n",
    "\n",
    "    return {\n",
    "        \"R@1\": recall_at_k(sim, 1),\n",
    "        \"R@5\": recall_at_k(sim, 5),\n",
    "        \"R@10\": recall_at_k(sim, 10),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_caption_transformer(image, encoder, decoder, vocab, max_len=30):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    bos = vocab.word2idx[\"<start>\"]\n",
    "    eos = vocab.word2idx[\"<end>\"]\n",
    "\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    memory = encoder(image)  # [1, N, D]\n",
    "\n",
    "    generated = torch.tensor([[bos]], device=device)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        logits = decoder(memory, generated)      # [1, t, V]\n",
    "        next_token = logits[:, -1].argmax(dim=-1, keepdim=True)  # [1,1]\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == eos:\n",
    "            break\n",
    "\n",
    "    words = []\n",
    "    for idx in generated.squeeze(0).tolist():\n",
    "        w = vocab.idx2word[idx]\n",
    "        if w in (\"<start>\", \"<pad>\"):\n",
    "            continue\n",
    "        if w == \"<end>\":\n",
    "            break\n",
    "        words.append(w)\n",
    "\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe55d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  test the model on a few images\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "for file_name in os.listdir(TEST_IMAGES_PATH)[:20]:\n",
    "    # Load image\n",
    "    img_path = f\"{TEST_IMAGES_PATH}/{file_name}\"\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    image = transform(image)\n",
    "    plt.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    caption = generate_caption_transformer(image, encoder, decoder, vocab)\n",
    "    print(f\"Image: {file_name}\\nCaption: {caption}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
