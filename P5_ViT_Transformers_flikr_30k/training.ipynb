{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dae2319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c742bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = \"..\\\\..\\\\datasets\\\\\"\n",
    "models = \"..\\\\..\\\\models\\\\\"\n",
    "\n",
    "IMAGES_PATH = f\"{datasets}/flickr30k/flickr30k_images/flickr30k_images\"  # Directory with training images\n",
    "CAPTIONS_PATH = f\"{datasets}/flickr30k/flickr30k_images/results.csv\"  # Caption file\n",
    "TEST_IMAGES_PATH = \"..\\\\test_images\"  # Directory with test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6873b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Update the folowing datset call class to handle flikr8k dataset formate\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "\n",
    "class FlickrDataset30K(Dataset):\n",
    "    def __init__(self, root, captions_path, vocab, transform=None, max_samples=None):\n",
    "        self.root = root\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "        image_captions = {}\n",
    "        with open(captions_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                # Skip header\n",
    "                if line.startswith(\"image_name\"):\n",
    "                    continue\n",
    "\n",
    "                # Split by '|'\n",
    "                parts = [p.strip() for p in line.split(\"|\")]\n",
    "                if len(parts) < 3:\n",
    "                    continue\n",
    "\n",
    "                img_name,_, caption = parts\n",
    "                if img_name not in image_captions:\n",
    "                        image_captions[img_name] = []\n",
    "                image_captions[img_name].append(caption)\n",
    "        # Load captions\n",
    "        self.image_captions = image_captions\n",
    "\n",
    "        self.image_ids = list(self.image_captions.keys())\n",
    "\n",
    "        if max_samples:\n",
    "            self.image_ids = self.image_ids[:max_samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vocab = self.vocab\n",
    "        img_name = self.image_ids[index]\n",
    "        img_path = os.path.join(self.root, img_name)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except:\n",
    "            # Skip corrupted image\n",
    "            return self.__getitem__((index + 1) % len(self.image_ids))\n",
    "        \n",
    "\n",
    "        # Randomly select one caption per image\n",
    "        caption = random.choice(self.image_captions[img_name])\n",
    "\n",
    "        tokens = nltk.tokenize.word_tokenize(caption)\n",
    "\n",
    "        caption_indices = [vocab.word2idx[\"<start>\"]]\n",
    "        caption_indices += [\n",
    "            vocab.word2idx.get(token, vocab.word2idx[\"<unk>\"])\n",
    "            for token in tokens\n",
    "        ]\n",
    "        caption_indices.append(vocab.word2idx[\"<end>\"])\n",
    "\n",
    "        caption_tensor = torch.tensor(caption_indices)\n",
    "\n",
    "        return image, caption_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aee3d968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import nltk \n",
    "from collections import Counter\n",
    "from vocabulary_class import Vocabulary\n",
    "nltk.download('punkt')\n",
    "import json\n",
    "\n",
    "tokens = []\n",
    "counter = Counter()\n",
    "\n",
    "import csv\n",
    "import string\n",
    "\n",
    "def build_vocab(captions_path, threshold=3, limit=None):\n",
    "    \n",
    "    counter = Counter()\n",
    "    image_captions = {}\n",
    "    count =0\n",
    "    with open(captions_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        # reader = csv.reader(f)\n",
    "        # next(reader)  # skip header: image,caption\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # Skip header\n",
    "            if line.startswith(\"image_name\"):\n",
    "                continue\n",
    "\n",
    "            # Split by '|'\n",
    "            parts = [p.strip() for p in line.split(\"|\")]\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "\n",
    "            img_name,_, caption = parts\n",
    "            if img_name not in image_captions:\n",
    "                    image_captions[img_name] = []\n",
    "            image_captions[img_name].append(caption)\n",
    "\n",
    "            caption = caption.lower()\n",
    "            tokens = [\n",
    "                token for token in nltk.tokenize.word_tokenize(caption)\n",
    "                if token not in string.punctuation\n",
    "            ]\n",
    "            if len(tokens) < 1:\n",
    "                continue\n",
    "            counter.update(tokens)\n",
    "            if limit and len(image_captions) >= limit:\n",
    "                break\n",
    "    \n",
    "    vocab = Vocabulary()\n",
    "    for word, cnt in counter.items():\n",
    "        if cnt >= threshold:\n",
    "            vocab.add_word(word)\n",
    "    \n",
    "\n",
    "    return vocab, image_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7bc8cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 8323\n",
      "max idx: 8322\n",
      "len(vocab): 8323\n"
     ]
    }
   ],
   "source": [
    "vocab, image_captions = build_vocab(CAPTIONS_PATH, threshold=3)\n",
    "print(\"Total vocabulary size:\", len(vocab))\n",
    "\n",
    "indices = list(vocab.word2idx.values())\n",
    "print(\"max idx:\", max(indices))\n",
    "print(\"len(vocab):\", len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78bfa16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22248"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f57a7116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22248\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 224, 224])\n",
      "tensor([ 1,  3, 27, 17, 28, 29, 22, 30, 17, 31, 19,  3,  2])\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    captions = pad_sequence(\n",
    "        captions,\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "\n",
    "    return images, captions\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "# from flickr_dataset  import FlickrDataset \n",
    "\n",
    "train_dataset = FlickrDataset30K(\n",
    "    root=IMAGES_PATH,\n",
    "    captions_path=CAPTIONS_PATH,\n",
    "    vocab=vocab,\n",
    "    transform=transform,\n",
    "    max_samples=None\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn= collate_fn\n",
    ")\n",
    "\n",
    "print(len(train_dataset))\n",
    "image, caption = train_dataset[0]\n",
    "\n",
    "print(type(image))\n",
    "print(image.shape)        # after transform\n",
    "print(caption)\n",
    "print(len(caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8966eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from model  import TransformerEncoderViT\n",
    "from model  import TransformerDecoder\n",
    "\n",
    "encoder = TransformerEncoderViT(embed_size=256).to(device)\n",
    "decoder = TransformerDecoder(embed_size=256, vocab_size=len(vocab)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[\"<pad>\"])\n",
    "\n",
    "params = filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params,\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5295912e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/696 [00:00<?, ?it/s]d:\\AIML\\envs\\mlenv\\lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "100%|██████████| 696/696 [03:36<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train=4.1688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [03:36<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train=3.3918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [03:36<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train=3.1435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [03:36<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train=2.9874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [03:36<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train=2.8855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [03:36<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train=2.7909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [03:37<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train=2.7174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [03:37<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train=2.6634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [03:37<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train=2.6137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [03:36<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train=2.5650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "for epoch in range(10):\n",
    "    total_train_loss = 0\n",
    "    for images, captions in tqdm.tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Teacher forcing:\n",
    "        # input:  <start> w1 w2 ... w(T-1)\n",
    "        # target: w1 w2 ... w(T-1) <end>\n",
    "        captions_in = captions[:, :-1]\n",
    "        targets     = captions[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        memory = encoder(images)\n",
    "        logits = decoder(memory, captions_in)   # <-- IMPORTANT\n",
    "        \n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(list(encoder.parameters()) + list(decoder.parameters()), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch}: Train={avg_train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fab1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), \"models/encoder.pth\")\n",
    "torch.save(decoder.state_dict(), \"models/decoder.pth\")\n",
    "torch.save(vocab, \"models/vocab.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
